[
  {
    "objectID": "aiandgraphs.html",
    "href": "aiandgraphs.html",
    "title": "Graphs and AI",
    "section": "",
    "text": "AI can be used to write code to solve problems you have with your data but you need to know what variables and numbers go where and exactly what you want to achieve.\nTreat AI like a peer - they could be right but they may have misunderstood.\nYou MUST credit AI software when you use it. Not doing so is a form of plagiarism.\nYou MUST NOT use AI to write text for your assignments!\n\n\nChallenge\nCreate an account with ChatGPT3.5 (freeversion), Google bard or similar AI. Use it in the challenges below to help you write code.\nTry to get the code to run in R. If it fails try to rephrase your prompt to the AI."
  },
  {
    "objectID": "aiandgraphs.html#graphs",
    "href": "aiandgraphs.html#graphs",
    "title": "Graphs and AI",
    "section": "Graphs",
    "text": "Graphs\nLoad the data (they come with R) then recreate the two graphs by finding code on the internet for the basic graph and building on it.\nLook at this Learning R webpage if you need an introduction or reminder on using ggplot to build graphs in R.\n\nChallenge\nUse this data:\n\ndata(mtcars)\n\nRecreate this graph using the variables miles per gallon mpg, weight wt, displacement disp and automatic or manual am.\n\n\n\nHints\n\nThis is a scatterplot. The size of the points are weighted by disp. Points are coloured by am.\nThere are many ways to write code to achieve this graph.\nIf something doesn’t work don’t be afraid to look for alternative code.\n\n\nChallenge\nUse the tooth growth dataset:\n\ndata(ToothGrowth)\n\nRecreate this graph using the variable len and dose.\n\n\n\nHints\n\nThis is called a jitter plot with errorbars.\nThe raw data is needed for the jitter part but the mean and standard deviation values are needed for the errorbars. Therefore you may need to summarise the data.\nR sometimes thinks dose is a continuous variable not a factor.\n\n\nChallenge\nUse the iris dataset:\n\ndata(iris)\n\nRecreate this graph using the variables species, sepal width and sepal length.\n\n\n\nHints\n\nThe first graph is a histogram, the second violin and jittered points and the third a scatter plot with a regression line.\nGraph arrangements like this might be called subplots, panels or grids.\n\n\n\n\nReflect on using AI\nDid the R code always work first time? Did you or the AI correct it?\nDid you ask the AI to address two problems at once or one at a time and what worked?\nCould you have been more specific in your question?\nDid you give the AI the raw data? Is it a good idea to give your data to AI software?\n\n\n\nCiting AI\nSome norms are emerging for correctly crediting AI.\nOne suggestion is to state in the methods section what part of the work you used AI for, what software was used and what you asked it to do.\nFor example you may write “The R code used to run a power analysis on the chaffinch data was written using artificial intelligence software ChatGPT 3.5 (OpenAI, 2023).\nThe reference would be “OpenAi. 2023. ChatGPT (26 Oct version) [Large language model]. https://chat.opanai.com/chat”"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Advanced Statistics",
    "section": "",
    "text": "{fig-alt = “Cute fuzzy monsters putting rectangular data tables onto a conveyor belt. Along the conveyor belt line are different automated “stations” that update the data, reading “WRANGLE”, “VISUALIZE”, and “MODEL”. A monster at the end of the conveyor belt is carrying away a table that reads “Complete analysis.” “}"
  },
  {
    "objectID": "lm.html",
    "href": "lm.html",
    "title": "Linear Models",
    "section": "",
    "text": "Linear models can be used to run regressions (where the response and predictor are both continuous) or t-tests and ANOVAs (where the response is continuous and the predictor is a factor.)"
  },
  {
    "objectID": "lm.html#running-the-analysis",
    "href": "lm.html#running-the-analysis",
    "title": "Linear Models",
    "section": "Running the analysis",
    "text": "Running the analysis\nIn R you can fit linear models using the function lm.\n\nlm(loght ~ temp, data = Plant_height)\n\nThe response variable loght goes before the tilde. After the tilde we list the predictor variables, only temp in this case.\nThe data = argument specifies the data frame from which the variables will be taken.\n\nTo obtain detailed output (e.g., coefficient values, R2, test statistics, p-values, confidence intervals etc.), assign the output of the lm function to a new object in R. Then pass that new model object through the summary function.\n\nmodel <- lm(loght ~ temp, data = Plant_height)\nsummary(model)\n\n\nCall:\nlm(formula = loght ~ temp, data = Plant_height)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.97903 -0.42804 -0.00918  0.43200  1.79893 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -0.225665   0.103776  -2.175    0.031 *  \ntemp         0.042414   0.005593   7.583 1.87e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6848 on 176 degrees of freedom\nMultiple R-squared:  0.2463,    Adjusted R-squared:  0.242 \nF-statistic:  57.5 on 1 and 176 DF,  p-value: 1.868e-12"
  },
  {
    "objectID": "lm.html#what-has-lm-just-done",
    "href": "lm.html#what-has-lm-just-done",
    "title": "Linear Models",
    "section": "What has lm just done?",
    "text": "What has lm just done?\nIt has tried to make a line of best fit (the blue line in the graph).\n\n\n\n\n\nThe equation for that line is in the form \\[y = \\alpha + \\beta x \\]\n\\(\\alpha\\) is the intercept (where the line crosses the y axis). \\(\\beta\\) is the slope. (This is the same as the equation for a straight line y = mx + c or y = ax + b you may have encountered before.)\nThe goal of lm is to obtain the best estimates for \\(\\alpha\\) and \\(\\beta\\). \\(\\alpha\\) and \\(\\beta\\) are called the model coefficients.\nTo make it a model rather than just a straight line, it also has an extra bit called the error term \\(\\varepsilon\\). You can think of this as how close the points are to the line. \\(\\varepsilon\\) is not usually reported as part of the equation. \\[y = \\alpha + \\beta x + \\varepsilon \\]"
  },
  {
    "objectID": "lm.html#interpreting-the-results",
    "href": "lm.html#interpreting-the-results",
    "title": "Linear Models",
    "section": "Interpreting the results",
    "text": "Interpreting the results\nThe output given by summary() gives us the \\(\\beta\\) and \\(\\alpha\\) coefficients so we can report the model equation \\[log(plant height) = -0.22566 +0.0421.temperature + \\varepsilon \\]\nLook at the output to find where these numbers came from.\n\nNote that \\(\\beta\\) which is the slope can be interpreted as the amount of change in \\(y\\) for each unit of \\(x\\). For example, as the temperature increases by 1 degree, the log(plant height) increases by 0.0241.\n\n\nPassing the model object through summary() also gives us the t-statistics and p-values related to each predictor. These test the null hypothesis that the true value for the coefficient is 0.\nFor the intercept we usually don’t care if it is zero or not, but for the other coefficient (the slope), a value significantly differing from zero indicates that there is an association between that predictor and the response. In this example, temperature affects plant height.\nWhilst the t-statistics and p-values indicate a significant association, the strength of the association is captured by the R2 value. R2 is the proportion of variance in the response that is explained by the predictor(s).\nThe F-statistic and associated p-value indicates whether the model as a whole is significant. The model will always be significant if any of the coefficients are significant. With only one predictor variable, the probability associated with the t test, that tests whether the slope differs from zero, is identical to the probability associated with the F statistic.\nWe can also obtain 95% confidence intervals for the two parameters. Checking that the intervals for the slope do not include zero is another way of showing that there is an association between the dependent and predictor variable.\n\nconfint(model)\n\n                  2.5 %      97.5 %\n(Intercept) -0.43047074 -0.02085828\ntemp         0.03137508  0.05345215\n\n\nIn summary, you could report\n\nThe model (log(plant height) = -0.22566 + 0.0421.temperature, R^2 = 0.246) was significant (F(1,176) = 57.5, p < 0.001) with temperature significantly predicting (t = 7.583, p < 0.001) the height of the plants. This means that when temperature increases by 1 degree the plant height increases by 0.042 (CI 0.031, 0.053).\n\nIf you have run several analyses (or if there is more than one predictor), it may be useful to present the results as a table with coefficient values, standard errors and p-values for each explanatory variable. What parts you choose to report is down to discipline, style of the journal or what the writer thinks should be emphasised to answer the results question.\n\n\nAssumptions to check\nBut to have confidence in our results we should check the data met the assumptions.\nIndependence. For all the data in these examples we’ll assume the observations are independent of each other.\n\nThere are a variety of measures for dealing with non-independence. These include ensuring all important predictors are in the model; averaging across nested observations; or using a mixed-model (covered in another lesson).\n\n\nLinearity. There is no point trying to fit a straight line to data that are curved!\nPassing model through plot() gives four graphs. The first is a plot of residuals versus fitted values. Curvilinear relationships produce patterns in such plots.\n\nplot(model)\n\nThe absence of strong patterning in the first plot indicates the assumption of linearity is valid.\nClick here to see what patterns of residuals you would expect with curved relationships\n\nConstant variance If the plot of residuals versus fitted values is fan-shaped, the assumption of constant variance (homogeneity of variance) is violated.\n\nNormality. Checks of whether the data are normally distributed are usually performed by either plotting a histogram of the residuals or via a quantile plot where the residuals are plotted against the values expected from a normal distribution (the second of the figures obtained by plot(model)). If the points in the quantile plot lie mostly on the line, the residuals are normally distributed.\n\nhist(model$residuals) # Histogram of residuals\n\n\n\nplot(model, which = 2) # Quantile plot\n\n\n\n\nProblems with variance or normality can be addressed via transformations or by using a Generalised Linear Model, GLM. Note, however, that linear regression is reasonably robust against violations of constant variance and normality."
  },
  {
    "objectID": "lm.html#visualising-data",
    "href": "lm.html#visualising-data",
    "title": "Linear Models",
    "section": "Visualising data",
    "text": "Visualising data\nWe could plot a boxplot or bar chart with overlayed points. An alternative is a violin plot using geom_violin.\n\n  ggplot(aes(x = River_name, y = pH), data = River_pH) +\n  geom_violin()\n\n\n\n\nOverlay the means and their 95% confidence intervals using stat_summary(). Change the axis labels using xlab() and ylab().\n\n  ggplot(aes(x = River_name, y = pH), data = River_pH) +\n  geom_violin() +\n  stat_summary(fun = \"mean\", size = 0.2) +\n  stat_summary(fun.data = \"mean_cl_normal\", geom = \"errorbar\", width = 0.2) + \n  xlab(\"River\") +\n  ylab(\"pH of River\")\n\n\n\n\n\n\nfun and fun.data explained\n\nfun and fun.data are arguments in stat_summary() that do statistical operations to data. fun takes the data and returns a single value such as the mean. fun.data calculates three values for each group: y, ymin and ymax. In our case, ymin is the lower confidence interval and ymax is the upper confidence interval.\n\n\nChallenge\nRead in the Palmer Penguins dataset (penguins.csv). Make a violin plot of body_weight_g for the two groups in sex.\nCan you search the internet to find out how to remove NA values?\n\n\nSolution\n\nRead in the data\n\npenguins <- read.csv(file = \"data/penguins.csv\")\n\nMake a violin plot with mean, error bars, and axes labels.\n\n  ggplot(aes(x = sex, y = body_mass_g), data = penguins) +\n  geom_violin() +\n  stat_summary(fun = \"mean\", size = 0.2) +\n  stat_summary(fun.data = \"mean_cl_normal\", geom = \"errorbar\", width = 0.2) + \n  xlab(\"Penguin Sex\") +\n  ylab(\"Body Mass (g)\")\n\n\n\n\nOne solution (of many) to remove NA values is piping the data into the drop_na() function from the tidyr package. The resulting data can be piped into ggplot.\n\nlibrary(tidyr)\npenguins %>% \n        drop_na(sex) %>%\n        ggplot(aes(x = sex, y = body_mass_g)) +\n  geom_violin() +\n  stat_summary(fun = \"mean\", size = 0.2) +\n  stat_summary(fun.data = \"mean_cl_normal\", geom = \"errorbar\", width = 0.2) + \n  xlab(\"Penguin Sex\") +\n  ylab(\"Body Mass (g)\")"
  },
  {
    "objectID": "lm.html#fitting-a-model",
    "href": "lm.html#fitting-a-model",
    "title": "Linear Models",
    "section": "Fitting a model",
    "text": "Fitting a model\nAs the previous example, use lm() and then put the resulting model through summary().\n\nmodel <- lm(pH ~ River_name, data = River_pH)\nsummary(model)\n\nlm() has used the same equation but since our predictor is a factor/category oppose to numeric, how we interpret the results is different.\nThere are two groups - A and B. One is taken by the model as the baseline (A), the other as the contrast (B). The first level alphabetically is chosen by R as the baseline.\nThe intercept in the output is the estimated mean for the baseline, i.e. for River A. The B estimate is the estimated mean difference in pH between River A and B. We can therefore write the equation for this model as:\n\\[pH = 8.6615 -2.2529 \\times x\\] where \\(x = 1\\) if the river is river B or \\(x = 0\\) if it is the baseline river A.\nWe could report: There is a significant difference in pH between river A (mean = 8.66) and river B (mean = 6.41; t = -6.98, p < 0.001).\n\n\nAre these results the same as running a t test?\n\nYes! Same t and p values.\n\nt.test(pH ~ River_name, data = River_pH, var.equal = TRUE)\n\n\n    Two Sample t-test\n\ndata:  pH by River_name\nt = 6.9788, df = 18, p-value = 1.618e-06\nalternative hypothesis: true difference in means between group A and group B is not equal to 0\n95 percent confidence interval:\n 1.574706 2.931168\nsample estimates:\nmean in group A mean in group B \n       8.661497        6.408560 \n\n\n\n\nChallenge\nRun a model and report if there is an effect of sex on the body_mass_g of penguins.\n\n\nSolution\n\n\nmodel <- lm(body_mass_g ~ sex, data = penguins)\nsummary(model)\n\nThere is a significant effect of sex on penguin body mass with males larger (mean = 4545.68g) than females (mean = 3862.27g; t = 8.54, p < 0.001).\n\n\nChallenge\nCheck the assumptions of the penguin sex model using plot.\nDo you think it meets the assumptions?\n\n\nSolution\n\n\nplot(model)\n\nA linear relationship is not relevant here as the predictor is categorical not numeric. Something is wrong with the normality of the residuals. This would alert us to some other variable effecting the data - in this case penguin species. The variance might be greater in males than females."
  },
  {
    "objectID": "lm.html#running-the-analysis-1",
    "href": "lm.html#running-the-analysis-1",
    "title": "Linear Models",
    "section": "Running the analysis",
    "text": "Running the analysis\nSave the turtle hatching data, Turtles.csv, import into R and check the temperature variable is a factor with the str function.\n\nTurtles <- read.csv(file = \"data/Turtles.csv\", header = TRUE)\nstr(Turtles)\n\n'data.frame':   40 obs. of  2 variables:\n $ Temperature: int  15 15 15 15 15 15 15 15 15 15 ...\n $ Days       : int  37 43 45 54 56 65 62 73 74 75 ...\n\n\nR is treating Temperature as a numeric (int means integer). We need to change that variable to become a factor (categories).\n\nTurtles$Temperature <- factor(Turtles$Temperature)\n\nNow run the model using lm.\n\nturtle_model <- lm(Days ~ Temperature, data = Turtles)\nsummary(turtle_model)\n\n\nCall:\nlm(formula = Days ~ Temperature, data = Turtles)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-28.200  -9.225   1.650   9.025  19.400 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)     58.400      4.092  14.273  < 2e-16 ***\nTemperature20  -13.800      5.787  -2.385   0.0225 *  \nTemperature25   -9.200      5.787  -1.590   0.1206    \nTemperature30  -38.300      5.787  -6.619 1.04e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 12.94 on 36 degrees of freedom\nMultiple R-squared:  0.5711,    Adjusted R-squared:  0.5354 \nF-statistic: 15.98 on 3 and 36 DF,  p-value: 9.082e-07\n\n\n\nIf we thought we needed a post hoc test we could pass our model object through emmeans() from emmeans package.\n\nlibrary(emmeans)\nemmeans(turtle_model, pairwise ~ Temperature)\n\n$emmeans\n Temperature emmean   SE df lower.CL upper.CL\n 15            58.4 4.09 36     50.1     66.7\n 20            44.6 4.09 36     36.3     52.9\n 25            49.2 4.09 36     40.9     57.5\n 30            20.1 4.09 36     11.8     28.4\n\nConfidence level used: 0.95 \n\n$contrasts\n contrast                      estimate   SE df t.ratio p.value\n Temperature15 - Temperature20     13.8 5.79 36   2.385  0.0983\n Temperature15 - Temperature25      9.2 5.79 36   1.590  0.3970\n Temperature15 - Temperature30     38.3 5.79 36   6.619  <.0001\n Temperature20 - Temperature25     -4.6 5.79 36  -0.795  0.8563\n Temperature20 - Temperature30     24.5 5.79 36   4.234  0.0008\n Temperature25 - Temperature30     29.1 5.79 36   5.029  0.0001\n\nP value adjustment: tukey method for comparing a family of 4 estimates"
  },
  {
    "objectID": "lm.html#assumptions-to-check-1",
    "href": "lm.html#assumptions-to-check-1",
    "title": "Linear Models",
    "section": "Assumptions to check",
    "text": "Assumptions to check\n\nplot(turtle_model)\n\n\n\n\n\n\n\n\n\n\n\n\nhist(turtle_model$residuals)\n\n\n\n\nRemember: the first graph produced by plot(), tells us about homogeneity of variance (equal variance). Look for an even spread of the residuals on the y axis for each of the levels on the x axis.\nThe second plot and the histogram from hist() tells us about normality."
  },
  {
    "objectID": "lm.html#interpreting-the-results-1",
    "href": "lm.html#interpreting-the-results-1",
    "title": "Linear Models",
    "section": "Interpreting the results",
    "text": "Interpreting the results\nChallenge\nGiven the output, write out how you could report these results. There will be many ways.\nHint: Look at how we reported the examples before. Look at how a paper in your discipline reported results. Look at how ANOVA is reported.\n\nChallenge\nRun a lm model to test the effect of penguin species on body_mass_g. Report the results.\n\n\nYou might have previously been taught to run an anova and post hoc Tukey test on continuous data with 3 or more factors. If you run those tests using the code below you get the same result.\n\n\nTurtle_aov <- aov(Days ~ Temperature, data = Turtles)\nsummary(Turtle_aov)\n\n            Df Sum Sq Mean Sq F value   Pr(>F)    \nTemperature  3   8025  2675.2   15.98 9.08e-07 ***\nResiduals   36   6027   167.4                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nTukeyHSD(Turtle_aov)\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = Days ~ Temperature, data = Turtles)\n\n$Temperature\n       diff       lwr        upr     p adj\n20-15 -13.8 -29.38469   1.784689 0.0982694\n25-15  -9.2 -24.78469   6.384689 0.3969971\n30-15 -38.3 -53.88469 -22.715311 0.0000006\n25-20   4.6 -10.98469  20.184689 0.8562615\n30-20 -24.5 -40.08469  -8.915311 0.0008384\n30-25 -29.1 -44.68469 -13.515311 0.0000785"
  },
  {
    "objectID": "lm.html#running-the-analysis-2",
    "href": "lm.html#running-the-analysis-2",
    "title": "Linear Models",
    "section": "Running the analysis",
    "text": "Running the analysis\nWe can fit a model to test whether the probability of crab presence changes with time (a factor) and distance (a continuous variable).\nThe response variable (presence/absence of crabs) is binomial, so we use family=binomial in the glm.\n\ncrab_glm <- glm(CrabPres ~ Time * Dist, family = \"binomial\", data = crabs)"
  },
  {
    "objectID": "lm.html#assumptions-to-check-2",
    "href": "lm.html#assumptions-to-check-2",
    "title": "Linear Models",
    "section": "Assumptions to check",
    "text": "Assumptions to check\nAssumption - There is a straight line relationship between the logit function of the mean of \\(y\\) and the predictors \\(x\\)\nFor this assumption, we check the residual plot for non-linearity, or a U-shape.\n\nplot(crab_glm, which = 1)\n\n\n\n\nUnfortunately, passing the glm object through the plot function gives us a very odd looking plot due to the discreteness of the data (i.e., many points on top of each other).\nFor a more useful plot we can instead fit the model using the manyglm() function in the mvabund package.\n\n\n\n\nlibrary(mvabund)\ncrab_manyglm <- manyglm(CrabPres ~ Time * Dist, family = \"binomial\", data = crabs)\nplot(crab_manyglm)\n\n\n\n\nIn our case there is no evidence of non-linearity.\nIf the residuals seem to go down then up, or up then down, we may need to add a polynomial function of the predictors using the poly function."
  },
  {
    "objectID": "lm.html#interpreting-the-results-2",
    "href": "lm.html#interpreting-the-results-2",
    "title": "Linear Models",
    "section": "Interpreting the results",
    "text": "Interpreting the results\nFor binomial models in particular the p-values from the summary function are not reliable, and we prefer to use the anova function to see if predictors are significant.\n\nsummary(crab_glm)\n\n\nCall:\nglm(formula = CrabPres ~ Time * Dist, family = \"binomial\", data = crabs)\n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)  \n(Intercept) -1.71431    0.68664  -2.497   0.0125 *\nTime10       1.29173    0.87194   1.481   0.1385  \nDist         0.02522    0.11137   0.226   0.8208  \nTime10:Dist  0.05715    0.14149   0.404   0.6863  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 71.097  on 56  degrees of freedom\nResidual deviance: 63.466  on 53  degrees of freedom\nAIC: 71.466\n\nNumber of Fisher Scoring iterations: 4\n\nanova(crab_glm, test = \"Chisq\")\n\nAnalysis of Deviance Table\n\nModel: binomial, link: logit\n\nResponse: CrabPres\n\nTerms added sequentially (first to last)\n\n          Df Deviance Resid. Df Resid. Dev Pr(>Chi)   \nNULL                         56     71.097            \nTime       1   6.6701        55     64.427 0.009804 **\nDist       1   0.7955        54     63.631 0.372448   \nTime:Dist  1   0.1647        53     63.466 0.684852   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe p-value for Time is P<0.01 so we conclude there is an effect of time on the presence of crabs, but no effect of distance or interaction between time and distance.\n\n\nanova results\n\nWhen there is more than one predictor, the maths ANOVA uses can be done in three different ways. These ways are named type I, II and III. This R bloggers article explains the differences.\nOur crab example is approximately balanced (even sample numbers in each group) so whatever version of ANOVA R uses we’ll get the same results. However, if you have unbalanced data you could compare differences among type I, II and III ANOVAs using the function Anova() in the car package.\n\nanova(lm_model) # default is type 1\ncar::Anova(lm_model, type = 2)\ncar::Anova(lm_model, type = 3)\n\n\n\n\nThis sample is reasonably large, so these p-values should be a good approximation. For a small sample it is often better to use resampling to calculate p-values. When you use manyglm the summary and anova functions use resampling by default.\nIn this case the results are quite similar, but in small samples it can often make a big difference.\n\n\n\n\nOptimising the model\n\nWhen there is more than one predictor you can try reducing the model by removing predictors and comparing models. We can use a number called the AIC to compare. Lower AICs are better.\n\nstep(crab_glm, test = \"Chi\")\n\nStart:  AIC=71.47\nCrabPres ~ Time * Dist\n\n            Df Deviance    AIC     LRT Pr(>Chi)\n- Time:Dist  1   63.631 69.631 0.16472   0.6849\n<none>           63.466 71.466                 \n\nStep:  AIC=69.63\nCrabPres ~ Time + Dist\n\n       Df Deviance    AIC    LRT Pr(>Chi)   \n- Dist  1   64.427 68.427 0.7955  0.37245   \n<none>      63.631 69.631                   \n- Time  1   70.275 74.275 6.6438  0.00995 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nStep:  AIC=68.43\nCrabPres ~ Time\n\n       Df Deviance    AIC    LRT Pr(>Chi)   \n<none>      64.427 68.427                   \n- Time  1   71.097 73.097 6.6701 0.009804 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nCall:  glm(formula = CrabPres ~ Time, family = \"binomial\", data = crabs)\n\nCoefficients:\n(Intercept)       Time10  \n     -1.609        1.535  \n\nDegrees of Freedom: 56 Total (i.e. Null);  55 Residual\nNull Deviance:      71.1 \nResidual Deviance: 64.43    AIC: 68.43\n\n\nstep() removes the interaction (Dist * Time), then Dist and the AIC improves (gets lower). This confirms they are not predictors of the response."
  },
  {
    "objectID": "lm.html#communicating-the-results",
    "href": "lm.html#communicating-the-results",
    "title": "Linear Models",
    "section": "Communicating the results",
    "text": "Communicating the results\nYou can use the p values to report results like in other tests, e.g., “There is strong evidence that the presence of crabs varies with time (p = 0.01).” For multiple predictors it’s best to display the results in a table.\n\n\nTip: People get stuck interpreting binomial results because they do not have a clear idea of what the baseline (reference) groups are in their models. In this example we would ensure we know that baseline for the response CrabPres is absence of crabs and baseline for Time is time point 5.\n\n\nThe coefficients for the intercept is the value of the response variable (on a logit scale) when the factor predictors (Time in our example) is the baseline (time point 5 in our example) and the numeric predictors (Dist) is 0. The coefficient for Time (a factor) tells us the difference in the response between the baseline and the other group of the factor (the difference between time point 5 and time point 10).\nThe coefficients for numeric predictors can show negative or positive relationships with the response.\nThe coefficient numbers (called log odds) are difficult for you (and your readers) to interpret. Many people convert them into effect sizes called odds ratios to report them.\n\nexp(coef(crab_glm)) # calculates the exponential of the coefficients in the model i.e. turns log odds into odds ratios\n\n(Intercept)      Time10        Dist Time10:Dist \n  0.1800871   3.6390656   1.0255455   1.0588183 \n\n\nOdds ratios above 1 mean crabs are more likely to be present (present is coded as 1 in the response CrabPres). Odds ratios below 1 mean crabs are less likely to be present.\nThe odds ratio for Time is 3.6. We report “Crabs are 3.6 times more likely to be present at time point 10 compared to time point 5”.\n\nTip if the odds ratio is below 1 try recoding the explanatory variables so that another group is the baseline.\n\n\nFor numeric predictors a positive odds ratio such as 3.21 would mean that a 1 unit increase in the predictor, increases the odds of the response being present by 3.21. However, our odds ratio for distance is negative which is more difficult to put into words and relate back to the research question. One solution is to express it as the % decrease. For example, (0.97–1) * 100 = -3%. Then we can write “Each additional increase of one in distance is associated with an 3% decrease in the odds of a crab being present.\nChallenge\nWhat plots do you think could be used to present this data?"
  },
  {
    "objectID": "lm.html#running-the-analysis-3",
    "href": "lm.html#running-the-analysis-3",
    "title": "Linear Models",
    "section": "Running the analysis",
    "text": "Running the analysis\n\nThis example has counts of different animal groups at control sites and sites where bush regeneration has been carried out (treatment). We will use only one group of animals - slugs (Soleolifera is the order name of terrestrial slugs) to see if the the bush regeneration activities have affected slug abundance.\nSave revegetation.csv and import into R and view the data.\n\nreveg <- read.csv(\"data/revegetation.csv\", header = T)\n\nIf you view the frequency histogram of the slug counts, you will see that it is very skewed, with many small values and few large counts.\n\nhist(reveg$Soleolifera)\n\n\n\n\n\nThe default distribution for count data is the Poisson. The Poisson distribution assumes the variance equals the mean. This is quite a restrictive assumption which ecological count data often violates. We may need to use the more flexible negative-binomial distribution instead.\n\n\nWe can use a GLM to test whether the counts of slugs (from the order Soleolifera) differ between control and regenerated sites. To fit the GLM, we will use the manyglm function instead of glm so we have access to more useful residual plots.\nTo fit the GLM, load the mvabund package then fit the following model:\n\nlibrary(mvabund)\nslug_glm <- manyglm(Soleolifera ~ Treatment, family = \"poisson\", data = reveg)\n\nTreatment is the predictor variable with two levels, control and revegetated."
  },
  {
    "objectID": "lm.html#assumptions-to-check-3",
    "href": "lm.html#assumptions-to-check-3",
    "title": "Linear Models",
    "section": "Assumptions to check",
    "text": "Assumptions to check\nBefore looking at the results, look at the residual plot to check the assumptions.\n\nplot(slug_glm)\n\n\n\n\nIt’s hard to say whether there is any non-linearity in this plot, this is because the predictor is binary (control vs revegetated).\nLooking at the mean-variance assumption, it does appear as though there is a fan shape. The residuals are more spread out on the right than the left - we call this overdispersion.\nThis tells us the mean-variance assumption of the Poisson is probably violated. We should try a different distribution. We can instead fit a negative-binomial distribution in manyglm by changing the family argument to family=\"negative binomial\".\n\nslug_glm2 <- manyglm(Soleolifera ~ Treatment, family = \"negative binomial\", data = reveg)\n\nLook again at the residual plot:\n\nplot(slug_glm2)\n\n\n\n\nThis seems to have improved the residual plot. There is no longer a strong fan shape, so we can go ahead and look at the results."
  },
  {
    "objectID": "lm.html#interpreting-the-results-3",
    "href": "lm.html#interpreting-the-results-3",
    "title": "Linear Models",
    "section": "Interpreting the results",
    "text": "Interpreting the results\nWe can use summary and anova.\n\nanova(slug_glm2)\n\nTime elapsed: 0 hr 0 min 0 sec\n\n\nAnalysis of Deviance Table\n\nModel: Soleolifera ~ Treatment\n\nMultivariate test:\n            Res.Df Df.diff   Dev Pr(>Dev)   \n(Intercept)     48                          \nTreatment       47       1 10.52    0.004 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nArguments: P-value calculated using 999 iterations via PIT-trap resampling.\n\nsummary(slug_glm2)\n\n\nTest statistics:\n                     wald value Pr(>wald)    \n(Intercept)               1.502     0.030 *  \nTreatmentRevegetated      3.307     0.001 ***\n--- \nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nTest statistic:  3.307, p-value: 0.001 \nArguments: P-value calculated using 999 resampling iterations via pit.trap resampling.\n\n\nBoth tests indicate treatment has an effect (p<0.01)."
  },
  {
    "objectID": "lm.html#communicating-the-results-1",
    "href": "lm.html#communicating-the-results-1",
    "title": "Linear Models",
    "section": "Communicating the results",
    "text": "Communicating the results\nYou could write “There is strong evidence of a positive effect of bush regeneration on the abundance of slugs from the order Soleolifera (p < 0.01)”. For multiple predictors it’s best to display the results in a table.\nYou should also indicate which distribution was used (e.g. negative-binomial) and if resampling was used. “We used a negative-binomial generalised linear model due to overdispersion evident in the data. Bootstrap resampling was used with 1000 resamples” (1000 is the default when using manyglm()).\nChallenge\nWhat graph could be used to visualise the differences in slug counts between control and revegetated sites.\n\n\nSolution\n\nThere are various solutions. Boxplot is one.\n\nboxplot(Soleolifera ~ Treatment, ylab = \"Count\", xlab = \"Treatment\", data = reveg)\n\n\n\n\n\n\n\nAdapted from EnvironmentalComputing and Herman et al., 2021 Statistical Analysis for Public Health: Simple linear regression"
  },
  {
    "objectID": "metaanalyses.html",
    "href": "metaanalyses.html",
    "title": "Meta-analyses",
    "section": "",
    "text": "There are many steps involve in a meta-analysis. For guidance on how to do a meta-analysis see (Nakagawa et al. 2017) which divides the process of meta-analysis into 10 questions.\nThis lesson only considers how to do the statistics part of the meta-analysis & publication bias."
  },
  {
    "objectID": "metaanalyses.html#metafor-for-meta-analysis",
    "href": "metaanalyses.html#metafor-for-meta-analysis",
    "title": "Meta-analyses",
    "section": "Metafor for meta-analysis",
    "text": "Metafor for meta-analysis\nFirst, install and load the metafor package.\n\nlibrary(metafor)\n\n\nHave a look at the data set named dat.curtis1998 included in the package.\n\ndat <- dat.curtis1998\nstr(dat)\n\n'data.frame':   102 obs. of  20 variables:\n $ id      : int  21 22 27 32 35 38 44 63 86 87 ...\n $ paper   : int  44 44 121 121 121 121 159 183 209 209 ...\n $ genus   : chr  \"ALNUS\" \"ALNUS\" \"ACER\" \"QUERCUS\" ...\n $ species : chr  \"RUBRA\" \"RUBRA\" \"RUBRUM\" \"PRINUS\" ...\n $ fungrp  : chr  \"N2FIX\" \"N2FIX\" \"ANGIO\" \"ANGIO\" ...\n $ co2.ambi: num  350 350 350 350 350 350 350 395 350 350 ...\n $ co2.elev: num  650 650 700 700 700 700 700 795 700 700 ...\n $ units   : chr  \"ul/l\" \"ul/l\" \"ppm\" \"ppm\" ...\n $ time    : int  47 47 59 70 64 50 730 365 365 365 ...\n $ pot     : chr  \"0.5\" \"0.5\" \"2.6\" \"2.6\" ...\n $ method  : chr  \"GC\" \"GC\" \"GH\" \"GH\" ...\n $ stock   : chr  \"SEED\" \"SEED\" \"SEED\" \"SEED\" ...\n $ xtrt    : chr  \"FERT\" \"FERT\" \"NONE\" \"NONE\" ...\n $ level   : chr  \"HIGH\" \"CONTROL\" \".\" \".\" ...\n $ m1i     : num  6.82 2.6 2.99 5.91 4.61 ...\n $ sd1i    : num  1.77 0.667 0.856 1.742 1.407 ...\n $ n1i     : int  3 5 5 5 4 5 3 3 20 16 ...\n $ m2i     : num  3.94 2.25 1.93 6.62 4.1 ...\n $ sd2i    : num  1.116 0.328 0.552 1.631 1.257 ...\n $ n2i     : int  5 5 5 5 4 3 3 3 20 16 ...\n\n\n\nThis data set is from the paper by Curtis and Wang (1998). They looked at the effect of increased CO\\(_2\\) on plant traits (mainly changes in biomass). There is experimental details (sometimes called moderators) including species and functional group. There is also means (m), standard deviations (sd) and sample sizes (n) for the control group (1) and experimental group (2) in the last few columns."
  },
  {
    "objectID": "metaanalyses.html#calculating-standardized-effect-sizes",
    "href": "metaanalyses.html#calculating-standardized-effect-sizes",
    "title": "Meta-analyses",
    "section": "Calculating ‘standardized’ effect sizes",
    "text": "Calculating ‘standardized’ effect sizes\nTo compare the effect of increased CO\\(_2\\) across multiple studies, we first need to calculate an effect size for each study - a metric that quantifies the difference between our control and experimental groups.\n\nEffect size reminder. If experiment 1 measures the weights of swans and geese and experiment 2 measures the weight of swans and robins, the effect size is bigger in experiment 2. There is a bigger effect of species on weight in experiment 2.\n\n\nThere are several ‘standardized’ effect sizes. When we have two groups to compare, we have a choice of two effect size statistics we could use.\n\nThe first choice is standardized mean difference (SMD also known as Cohen’s \\(d\\) or Hedge’s \\(d\\) or \\(g\\); there are some subtle differences between d and g, but we do not worry about them for now.)\n\n\n\nFormula for calculating SMD\n\nIn the formula below:\n* \\(\\bar{x}_{C}\\) and \\(\\bar{x}_{E}\\) are the means of the control and experimental group, respectively\n* \\(sd\\) is sample standard deviation\n* \\(n\\) is sample size\n\\[\\begin{equation}\n\\mathrm{SMD}=\\frac{\\bar{x}_{E}-\\bar{x}_{C}}{\\sqrt{\\frac{(n_{C}-1)sd^2_{C}+(n_{E}-1)sd^2_{E}}{n_{C}+n_{E}-2}}}\n\\end{equation}\\]\n\n\nWe also need to calculate the SMD’s sample error variance for each study.\n\n\nFormula for calculating sample error variance\n\n\\[\\begin{equation}\nse^2_{\\mathrm{SMD}}= \\frac{n_{C}+n_{E}}{n_{C}n_{E}}+\\frac{\\mathrm{SMD}^2}{2(n_{C}+n_{E})}\n\\end{equation}\\]\n\n\nThe square root of this is referred to as ‘standard error’. The inverse of this (the inverse of a number is when you divide 1 by it e.g.\\(1/se^2\\)) is used as ‘weight’ (studies with bigger sample sizes will have bigger ‘weight’ in the analysis)\n\n\nThe second option of standardised effect size is called ‘response ratio’, which is usually presented in its natural logarithm form (lnRR).\n\n\n\nFormula for lnRR\n\n\\[\\begin{equation}\n\\mathrm{lnRR}=\\ln\\left({\\frac{\\bar{x}_{E}}{\\bar{x}_{C}}}\\right)\n\\end{equation}\\]\n\n\nThe sampling error variance for lnRR is also needed.\n\n\nFormula for calculating sample error variance for lnRR\n\n\\[\\begin{equation}\nse^2_\\mathrm{lnRR}=\\frac{sd^2_{C}}{n_{C}\\bar{x}^2_{C}}+\\frac{sd^2_{E}}{n_{E}\\bar{x}^2_{E}}\n\\end{equation}\\]\n\n\nWe can get R to calculate these numbers for each study using the function escalc() in metafor. To obtain the standardised mean difference (SMD), we use:\n\n# SMD\nSMD <- escalc(\n  measure = \"SMD\", n1i = dat$n1i, n2i = dat$n2i,\n  m1i = dat$m1i, m2i = dat$m2i,\n  sd1i = dat$sd1i, sd2i = dat$sd2i\n)\n\n\n\nCode Explained\n\nNote: in the example dataset dat the columns for the sample sizes, means and standard deviation have been given the same name as the arguments that are used in escalc()\n\nn1i and n2i are the sample sizes\nm1i and m2i are the means\nsd1i and sd2i the standard deviations from each study\n\n\n\nThe SMD object created has an effect size (yi) and its variance (vi) for each study.\n\n\n\n       yi     vi \n1  1.8222 0.7408 \n2  0.5922 0.4175 \n3  1.3286 0.4883 \n4 -0.3798 0.4072 \n5  0.3321 0.5069 \n6  2.5137 0.9282 \n\n\n\nTo obtain the alternative response ratio (lnRR), we would change the measure =:\n\nlnRR <- escalc(\n  measure = \"ROM\", n1i = dat$n1i, n2i = dat$n2i,\n  m1i = dat$m1i, m2 = dat$m2i,\n  sd1i = dat$sd1i, sd2i = dat$sd2i\n)\n\n\nThe original paper used lnRR so we will use it, but you will repeat the analysis in the Challenge below using SMD to see whether results are consistent.\n\nAdd the effect sizes to the original data set with bind_cols() from the package dplyr (some people use cbind() for this).\n\nlibrary(dplyr)\ndat <- bind_cols(dat, lnRR)\n\n\nYou should see yi (effect size) and vi (sampling variance) are added.\n\n\n'data.frame':   102 obs. of  22 variables:\n $ id      : int  21 22 27 32 35 38 44 63 86 87 ...\n $ paper   : int  44 44 121 121 121 121 159 183 209 209 ...\n $ genus   : chr  \"ALNUS\" \"ALNUS\" \"ACER\" \"QUERCUS\" ...\n $ species : chr  \"RUBRA\" \"RUBRA\" \"RUBRUM\" \"PRINUS\" ...\n $ fungrp  : chr  \"N2FIX\" \"N2FIX\" \"ANGIO\" \"ANGIO\" ...\n $ co2.ambi: num  350 350 350 350 350 350 350 395 350 350 ...\n $ co2.elev: num  650 650 700 700 700 700 700 795 700 700 ...\n $ units   : chr  \"ul/l\" \"ul/l\" \"ppm\" \"ppm\" ...\n $ time    : int  47 47 59 70 64 50 730 365 365 365 ...\n $ pot     : chr  \"0.5\" \"0.5\" \"2.6\" \"2.6\" ...\n $ method  : chr  \"GC\" \"GC\" \"GH\" \"GH\" ...\n $ stock   : chr  \"SEED\" \"SEED\" \"SEED\" \"SEED\" ...\n $ xtrt    : chr  \"FERT\" \"FERT\" \"NONE\" \"NONE\" ...\n $ level   : chr  \"HIGH\" \"CONTROL\" \".\" \".\" ...\n $ m1i     : num  6.82 2.6 2.99 5.91 4.61 ...\n $ sd1i    : num  1.77 0.667 0.856 1.742 1.407 ...\n $ n1i     : int  3 5 5 5 4 5 3 3 20 16 ...\n $ m2i     : num  3.94 2.25 1.93 6.62 4.1 ...\n $ sd2i    : num  1.116 0.328 0.552 1.631 1.257 ...\n $ n2i     : int  5 5 5 5 4 3 3 3 20 16 ...\n $ yi      : num  0.547 0.143 0.438 -0.113 0.117 ...\n  ..- attr(*, \"ni\")= int [1:102] 8 10 10 10 8 8 6 6 40 32 ...\n  ..- attr(*, \"measure\")= chr \"ROM\"\n $ vi      : num  0.0385 0.0175 0.0328 0.0295 0.0468 ..."
  },
  {
    "objectID": "metaanalyses.html#forest-plots",
    "href": "metaanalyses.html#forest-plots",
    "title": "Meta-analyses",
    "section": "Forest Plots",
    "text": "Forest Plots\nForest plots are a common way of visualising the effect sizes and their 95% confidence intervals, also known as CIs, (based on sampling error variance) for each of the studies in the meta-analysis. The forest() function can create this plot.\n\nforest(dat$yi, dat$vi)\n\n\n\n\n\nUnless you have a large screen, you may not be able to see the detail in this forest plot. Let’s plot just the first 12 studies.\n\nforest(dat$yi[1:12], dat$vi[1:12])\n\n\n\n\n\n\nWe can calculate many different kinds of effect sizes with escalc; other common effect size statistics include \\(Zr\\) (Fisher’s z-transformed correlation) which you would use if the meta-analysis was analysing studies that reported correlations oppose to comparing two groups.\n\n\nChallenge\nDo an internet search to find out how to interpret forest plots. What do the squares mean? Why is there a dotted line at 0? Some forest plots you see on the internet might have a diamond. What does the diamond represent?\n\nChallenge\nNow add the SMD values (the alternative ones to lnRR) to dat and create a forest plot with them. Compare the two forest plots.\n\n\nSolution\n\nUse the SMD object created above. Add the vi and yi columns in SMD to the dataset dat.\n\ndat <- bind_cols(dat, SMD)\n\nIf you view dat or run str(dat) you will see R has renamed the vi and yi columns. This means you have the lnRR values and then the SMD values.\n\nstr(dat)\n\n'data.frame':   102 obs. of  24 variables:\n $ id      : int  21 22 27 32 35 38 44 63 86 87 ...\n $ paper   : int  44 44 121 121 121 121 159 183 209 209 ...\n $ genus   : chr  \"ALNUS\" \"ALNUS\" \"ACER\" \"QUERCUS\" ...\n $ species : chr  \"RUBRA\" \"RUBRA\" \"RUBRUM\" \"PRINUS\" ...\n $ fungrp  : chr  \"N2FIX\" \"N2FIX\" \"ANGIO\" \"ANGIO\" ...\n $ co2.ambi: num  350 350 350 350 350 350 350 395 350 350 ...\n $ co2.elev: num  650 650 700 700 700 700 700 795 700 700 ...\n $ units   : chr  \"ul/l\" \"ul/l\" \"ppm\" \"ppm\" ...\n $ time    : int  47 47 59 70 64 50 730 365 365 365 ...\n $ pot     : chr  \"0.5\" \"0.5\" \"2.6\" \"2.6\" ...\n $ method  : chr  \"GC\" \"GC\" \"GH\" \"GH\" ...\n $ stock   : chr  \"SEED\" \"SEED\" \"SEED\" \"SEED\" ...\n $ xtrt    : chr  \"FERT\" \"FERT\" \"NONE\" \"NONE\" ...\n $ level   : chr  \"HIGH\" \"CONTROL\" \".\" \".\" ...\n $ m1i     : num  6.82 2.6 2.99 5.91 4.61 ...\n $ sd1i    : num  1.77 0.667 0.856 1.742 1.407 ...\n $ n1i     : int  3 5 5 5 4 5 3 3 20 16 ...\n $ m2i     : num  3.94 2.25 1.93 6.62 4.1 ...\n $ sd2i    : num  1.116 0.328 0.552 1.631 1.257 ...\n $ n2i     : int  5 5 5 5 4 3 3 3 20 16 ...\n $ yi...21 : num  0.547 0.143 0.438 -0.113 0.117 ...\n  ..- attr(*, \"ni\")= int [1:102] 8 10 10 10 8 8 6 6 40 32 ...\n  ..- attr(*, \"measure\")= chr \"ROM\"\n $ vi...22 : num  0.0385 0.0175 0.0328 0.0295 0.0468 ...\n $ yi...23 : num  1.822 0.592 1.329 -0.38 0.332 ...\n  ..- attr(*, \"ni\")= int [1:102] 8 10 10 10 8 8 6 6 40 32 ...\n  ..- attr(*, \"measure\")= chr \"SMD\"\n $ vi...24 : num  0.741 0.418 0.488 0.407 0.507 ...\n\n\nNow create a forest plot.\n\nforest(dat$yi...23, dat$vi...24)\n\n\n\n\nIf you want to view the previous forest plot for comparison try the blue arrow under the Plots tab. Alternative you could assign the plots and use that name to call them and view them.\n\nplotlnRR <- forest(dat$yi...21, dat$vi...22)\nplotlnRR\nplotSMD <- forest(dat$yi...23, dat$vi...24)\nplotSMD\n\nComparing the forest plots, there does seem to be differences. Keep in mind that you may want to investigate how using lnRR or SMD may or may not affect the analysis further on.\n\n\nBe careful that you don’t end up with two lots of yi and vi values since you have already created these when you did the lnRR methods. Don’t get confused!"
  },
  {
    "objectID": "metaanalyses.html#common-effect-model",
    "href": "metaanalyses.html#common-effect-model",
    "title": "Meta-analyses",
    "section": "Common-effect model",
    "text": "Common-effect model\nThis model estimates the overall mean while considering weights. Weights are used so that different studies with smaller or larger sample sizes have less or more influence in the calculation of the overall effect size.\n\n\nCommon-effect model formula and explanation\n\n\\[\\begin{equation}\ny_i=b_0+e_i,\n\\\\\ne_i\\sim \\mathcal{N}(0,v_i),\n\\end{equation}\\]\nwhere \\(y_i\\) is the \\(i\\)th effect size (from \\(i\\)th study), \\(b_0\\) is the overall mean (or meta-analytic mean), \\(e_i\\) is a deviation from the overall mean. \\(e_i\\) is equivalent to a normal distribution with a mean of 0 and variance of \\(v_i\\). \\(v_i\\) is the study specific sampling variance. Note that weights for this model are \\(1/v_i\\).\n\n\nThis model assumes that all the studies sampled from the same population and therefore there is a common mean for all studies. For example, all studies used the same species. This is rare in meta-analysis as the Curtis and Wang 1998 data shows where studies span many different species."
  },
  {
    "objectID": "metaanalyses.html#random-effects-model",
    "href": "metaanalyses.html#random-effects-model",
    "title": "Meta-analyses",
    "section": "Random-effects model",
    "text": "Random-effects model\nA random-effect model does not make this assumption and therefore can be used when studies have been sampled from different populations.\n\n\nRandom-effect model formula and explanation\n\n\\[\\begin{equation}\ny_i=b_0+s_i+e_i,\n\\\\\ns_i\\sim \\mathcal{N}(0,\\tau^2),\\\n\\\\\ne_i\\sim \\mathcal{N}(0,v_i),\n\\end{equation}\\]al{N}(0,v_i), \\end{equation}\nwhere \\(s_i\\) is a study-specific deviation from the overall mean for \\(i\\)th study. As the second formula indicates, \\(s_i\\) is normally distributed with the between-study variance which is \\(\\tau^2\\). Note that weights for this model are \\(1/(\\tau^2+v_i)\\). We revisit this point.\n\n\nUnlike the common-effect model, a random-effect model assumes that different studies have different population means."
  },
  {
    "objectID": "metaanalyses.html#running-a-common-effect-model",
    "href": "metaanalyses.html#running-a-common-effect-model",
    "title": "Meta-analyses",
    "section": "Running a common-effect model",
    "text": "Running a common-effect model\nLet’s use the function rma from metafor to run a common-effect model using the effect sizes yi and variances vi we calculated earlier. (Note: make sure those columns are still called yi and vi.)\n\n\n\n\ncommon_m <- rma(yi = yi, vi = vi, method = \"FE\", data = dat)\n\nWe specify the effect size (yi), its variance (vi), the method (“FE” for fixed-effect) and the data frame (dat).\n\nTo see the output, use summary on the model object:\n\nsummary(common_m)\n\n\nFixed-Effects Model (k = 102)\n\n   logLik   deviance        AIC        BIC       AICc   \n-245.9580   769.0185   493.9160   496.5410   493.9560   \n\nI^2 (total heterogeneity / total variability):   86.87%\nH^2 (total variability / sampling variability):  7.61\n\nTest for Heterogeneity:\nQ(df = 101) = 769.0185, p-val < .0001\n\nModel Results:\n\nestimate      se     zval    pval   ci.lb   ci.ub      \n  0.2088  0.0054  38.3374  <.0001  0.1982  0.2195  *** \n\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe overall mean is statistically significant (under Model Results look at pval). This indicates it is significantly different from 0 and therefore there is an effect of the CO\\(_2\\) treatment on plant biomass.\nThe overall mean is under estimate and it’s around 0.2. What does 0.2 mean? The effect sizes were response ratios on a logarithmic scale (lnRR). We can use exp() to convert this back into a response ratio of the control and experimental means.\n\nexp(0.2)\n\n[1] 1.221403\n\n\nWe can say that the plant trait (i.e. biomass) was 22% larger in the experimental group (RR\\(=\\bar{x}_{E}/\\bar{x}_{C}\\)), which is a pretty large effect (remember to interpret results in a biological meaningful way)."
  },
  {
    "objectID": "metaanalyses.html#running-a-random-effects-model",
    "href": "metaanalyses.html#running-a-random-effects-model",
    "title": "Meta-analyses",
    "section": "Running a random-effects model",
    "text": "Running a random-effects model\nNow, we move onto the random-effects model - a more realistic model because these studies were on different species. Again, we use the rma function, but this time change the method to REML which is the default and the best method for the random-effect meta-analysis.\n\nrandom_m <- rma(yi = yi, vi = vi, method = \"REML\", data = dat)\nsummary(random_m)\n\n\nRandom-Effects Model (k = 102; tau^2 estimator: REML)\n\n  logLik  deviance       AIC       BIC      AICc   \n  7.0449  -14.0898  -10.0898   -4.8596   -9.9674   \n\ntau^2 (estimated amount of total heterogeneity): 0.0262 (SE = 0.0053)\ntau (square root of estimated tau^2 value):      0.1619\nI^2 (total heterogeneity / total variability):   88.90%\nH^2 (total variability / sampling variability):  9.01\n\nTest for Heterogeneity:\nQ(df = 101) = 769.0185, p-val < .0001\n\nModel Results:\n\nestimate      se     zval    pval   ci.lb   ci.ub      \n  0.2553  0.0198  12.8899  <.0001  0.2165  0.2941  *** \n\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCompare the overall mean from this model with the common-effect model. Oh, the overall mean of the random-effects model is actually bigger than that of the fixed-effect model! OK, that sometimes happens (we will find out that this probably is an over-estimation later in the publication bias section).\nWe expect the 95% CI (under ci.lb and ci.ub) to be wider (i.e. more realistic) in this random-effects model as this model has a better assumption than the common-effect model."
  },
  {
    "objectID": "metaanalyses.html#understanding-heterogeneity",
    "href": "metaanalyses.html#understanding-heterogeneity",
    "title": "Meta-analyses",
    "section": "Understanding heterogeneity",
    "text": "Understanding heterogeneity\nThere are other numbers in the output. We have tau^2 (\\(\\tau^2\\)) and I^2 (\\(I^2\\)), two very common measures of heterogeneity (note that H^2, or \\(H^2\\) is a transformation of \\(I^2\\)).\n\nHeterogeneity is variation in effect sizes, which is not accounted for by the sampling error variance/random chance. In other words, how consistent the results are across all the studies. This is real variation in the data.\n\n\n\\(I^2\\) is an important index as it can tell the percentage of real variation in your meta-analytic data.\n\n\nFormula for \\(I^2\\)\n\n\\[\\begin{equation}\nI^2=\\frac{\\tau^2}{(\\tau^2+\\bar{v})},\n\\end{equation}\\]\nwhere \\(\\bar{v}\\) is a representative value of \\(v_i\\) (or think \\(\\bar{v}\\) as the average of \\(v_i\\) although it is not quite it). Note that the denominator is the whole variance which exists in the data.\n\n\nThe benchmark values for \\(I^2\\) are 25, 50 and 75% for low, moderate and high heterogeneity, respectively (Higgins et al., 2003.)\nOur \\(I^2\\) value is 88.9% so very high. The output also shows a Test for Heterogeneity or a \\(Q\\) test. As you might expect, \\(I^2\\) is statistically significant meaning there is heterogeniety.\n\nSenior et al. 2016 did a meta-analysis of meta-analyses looking at the average value of \\(I^2\\) in the field of ecology and evolution. The average value was 92%! This indicates that we usually need to fit the random-effects model rather than the common-effect model because the latter assumes heterogeneity to be zero or \\(\\tau^2=0\\) and \\(I^2 = 0\\). Or is it really? We find this out later."
  },
  {
    "objectID": "metaanalyses.html#funnel-plot",
    "href": "metaanalyses.html#funnel-plot",
    "title": "Meta-analyses",
    "section": "Funnel plot",
    "text": "Funnel plot\nTo create a funnel plot:\n\nfunnel(random_m)\n\n\n\n\nThe x axis is effect size. The overall effect size is plotted as a dotted vertical line. Each point shows a study’s effect size and standard error on the y axis. Note the y axis has 0 at the top.\nWhat am I talking about by ‘funnel asymmetry’? We expect to see a symmetrical up-side-down funnel, where effect sizes with low \\(se\\) are more tightly clustered than effect sizes with high \\(se\\). But if we have publication bias, we should see funnel asymmetry. This is because studies with small sample sizes (i.e. high \\(se\\), which leads to non-significance) are less likely to be published."
  },
  {
    "objectID": "metaanalyses.html#eggers-test",
    "href": "metaanalyses.html#eggers-test",
    "title": "Meta-analyses",
    "section": "Egger’s test",
    "text": "Egger’s test\nTo run Egger’s test:\n\n# Note that the orignal Egger's test is regtest(random_m, model=\"lm\")\nregtest(random_m)\n\n\nRegression Test for Funnel Plot Asymmetry\n\nModel:     mixed-effects meta-regression model\nPredictor: standard error\n\nTest for Funnel Plot Asymmetry: z = 3.2046, p = 0.0014\nLimit Estimate (as sei -> 0):   b = 0.1584 (CI: 0.0890, 0.2278)\n\n\nThe Egger’s test p value is significant suggesting asymetry. But we need to be careful. Funnel asymmetry can be caused not only by publication bias, but also by heterogeneity (one or more undetected moderators are distorting a funnel shape). Given we have a lot of unexplained variance (i.e. heterogeneity), we cannot be sure what is causing this asymmetry."
  },
  {
    "objectID": "metaanalyses.html#trim-and-fill",
    "href": "metaanalyses.html#trim-and-fill",
    "title": "Meta-analyses",
    "section": "Trim-and-fill",
    "text": "Trim-and-fill\nWe can use the alternative trim-and-fill method through the function trimfill(). We get a funnel plot by passing the result through funnel()\n\ntf_m <- trimfill(random_m)\ntf_m\n\n\nEstimated number of missing studies on the left side: 13 (SE = 6.5629)\n\nRandom-Effects Model (k = 115; tau^2 estimator: REML)\n\ntau^2 (estimated amount of total heterogeneity): 0.0421 (SE = 0.0076)\ntau (square root of estimated tau^2 value):      0.2053\nI^2 (total heterogeneity / total variability):   92.06%\nH^2 (total variability / sampling variability):  12.59\n\nTest for Heterogeneity:\nQ(df = 114) = 872.7669, p-val < .0001\n\nModel Results:\n\nestimate      se    zval    pval   ci.lb   ci.ub      \n  0.2166  0.0227  9.5234  <.0001  0.1721  0.2612  *** \n\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nfunnel(tf_m)\n\n\n\n\nAs you can see this method uses the asymmetry to add more points and provide a revised overall mean, which is smaller than that of the original random-effect model. Although this effect is still significant, this method could turn a significant overall mean into a non-significant one. But rather than taking this as a real estimate of the overall mean, we need to see this as a part of our sensitivity analysis.\n\n\nSensitivity analyses involve various statistical methods that test how the overall effect size changes depending on the decisions made during the meta-analysis. These decisions will have affected what studies and data was included in the calculation.\n\n\nThere are more methods for publication bias tests, none of which are perfect, but it is important to do some of these tests (for more see Nakagawa et al., 2017 and references therein)."
  },
  {
    "objectID": "mixedmodels1.html",
    "href": "mixedmodels1.html",
    "title": "Mixed Models 1",
    "section": "",
    "text": "We will use a fictional study system - dragons! If you are using R for yourself, click this link to the dragon data and right click to save it in your data file in your R project.\n\nImagine we went to eight mountain ranges (mountainRange) and collected data on the intelligence (testScore) and size (bodyLength) of 480 dragons. We want to know if size affects their intelligence since we want intelligent dragons that we can train but that aren’t too big and scary!\nLoad the csv dataset\n\ndragons <- read.csv(file = \"data/dragons.csv\")\nhead(dragons)\n\n  testScore bodyLength mountainRange\n1 16.147309   165.5485      Bavarian\n2 33.886183   167.5593      Bavarian\n3  6.038333   165.8830      Bavarian\n4 18.838821   167.6855      Bavarian\n5 33.862328   169.9597      Bavarian\n6 47.043246   168.6887      Bavarian\n\n\n\nOne way to analyse this data would be to fit a linear model.\nFit the model with testScore as the response and bodyLength as the predictor and have a look at the output:\n\nbasic.lm <- lm(testScore ~ bodyLength, data = dragons)\nsummary(basic.lm)\n\n\nCall:\nlm(formula = testScore ~ bodyLength, data = dragons)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-56.962 -16.411  -0.783  15.193  55.200 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -61.31783   12.06694  -5.081 5.38e-07 ***\nbodyLength    0.55487    0.05975   9.287  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 21.2 on 478 degrees of freedom\nMultiple R-squared:  0.1529,    Adjusted R-squared:  0.1511 \nF-statistic: 86.25 on 1 and 478 DF,  p-value: < 2.2e-16\n\n\nView the ‘Code Explained’ if using lm is new to you.\n\n\nCode Explained\n\n\nbasic.lm is the name we gave to the model object. We could have chosen to name it anything.\nlm() is the function that runs the linear model\ntestScore is the name of our variable that we want to be the response in the model.\nThe tidle ~ separates the response and predictors in the lm code.\nbodyLength is the name of the column of data that we want as the predictor in the model\ndata = is an argument in the lm function\ndragons is the name of the dataset that we want R to use.\n\n\n\nChallenge\nWrite out how you could report the output from the summary. (You may want to look back at the Linear Models lesson.)\n\n\nAnswer\n\nPoints you could take from the output include:\n* The coefficient estimate for bodyLength suggests that the model predicts testScore to increase by 0.55 for an increase of 1 in bodyLength.\n* The coefficient - standard error for bodyLength suggest the testScore can vary by 0.06.\n* The p value under Pr(>|t|) is significant indicating we might reject the null hypothesis that there is no relationship between bodyLength and testScore.\n* R2 is 0.1529 suggesting that 15.29% of the variance in testScore is explained by bodyLength.\n\n\nLet’s plot the data with ggplot2:\n\nlibrary(tidyverse)  # load the package containing both ggplot2 and dplyr\n\n\ndragonPlot <- ggplot(dragons, aes(x = bodyLength, y = testScore)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n\nOkay, both the linear model and the plot, suggest bigger dragons do better in our intelligence test. From our knowledge of dragons, that seems a bit odd: size shouldn’t really affect the test scores.\nBut are the assumptions met?\n\nThe plot above and below suggest we roughly meet the assumption of linearity.\n\nplot(basic.lm, which = 1)  # This is not perfect so we'd need to keep this in mind. For your own data be careful:the bigger the sample size, the less of a trend you'd expect to see.\n\n\n\n\n\nNext, check the assumption that the residuals are normal:\n\nplot(basic.lm, which = 2)  # some deviations at the ends but this is generally fine\n\n\n\n\n\nThen check the assumption of homoscedasticity (equal variance of residuals):\n\nplot(basic.lm, which = 3)  # a bit off but again doesn't look too bad\n\n\n\n\n\nBut another assumption of a linear model is independent observations.\nThis brings us to…\n\n\n\n\nConsider the description of the dragon study again (repeated below).\n\nImagine we went to eight mountain ranges (mountainRange) and collected data on the intelligence (testScore) and size (bodyLength) of 480 dragons.\n\nChallenge\nWhat is it that may not be right about analysing the data using a lm. Type your thoughts in your R script.\n\n\nAnswer\n\nThe analysis has not considered that there could be differences in the dragons among the eight different mountain ranges.\nThe dragons can be grouped by mountain range. Therefore, the dragons (and data) are not independent.\n\n\n\nBe aware that the word independent in statistics can be used to describe 1) independent data as well as 2) independent variables also known as predictors or factors. Confusingly, these are two different concepts.\n\n\nIt’s possible that the dragons from within each mountain range are more similar to each other than the dragons from different mountain ranges.\nHave a look at a boxplot of the data to see if this is true:\n\nboxplot(testScore ~ mountainRange, data = dragons)  # Looks like something is going on here. The median test score for different mountain ranges is different.\n\n\n\n\nWe could also create a scatterplot and colour points by mountain range:\n\n(colour_plot <- ggplot(dragons, aes(x = bodyLength, y = testScore, colour = mountainRange)) +\n  geom_point(size = 2) +\n  theme_classic() +\n  theme(legend.position = \"none\"))\n\n\n\n\nFrom the above plots, it looks like our mountain ranges vary both in the dragon body length AND in their test scores. This confirms that our observations from within each of the ranges aren’t independent. We can’t ignore that: it could lead to a completely erroneous conclusion!\nSo what do we do?\nWe could run eight separate analyses and fit a regression for each of the mountain ranges.\nLets have a quick look at the data split by mountain range. We use the facet_wrap() to do that:\n\n(split_plot <- ggplot(aes(bodyLength, testScore), data = dragons) + \n  geom_point() + \n  facet_wrap(~ mountainRange) + # create a facet for each mountain range\n  xlab(\"length\") + \n  ylab(\"test score\"))\n\n\n\n\nDoing eight analyses increases our chance of a Type 1 error. It also decreases the sample size from 480 dragons to 60. Not ideal!\nWe want to use all the data, but control for the data coming from different mountain ranges. We are not interested in quantifying test scores for each specific mountain range. This means we could use mountainRange as a random effect in a mixed model.\nThe explanatory variable bodyLength will be the fixed effect in the mixed model.\n\n\nThe word mixed in mixed model refers to the mix of random and fixed effects.\nDo not be misled by the use of the word random for random effect. It does not mean that the variable is mathematically random in anyway.\n\n\n\n\n\nMixed models are run using the lmer() (linear mixed effect regression) function in the package lme4.\nLibrary load the package ensuring it is installed first:\n\nlibrary(lme4)\n\nA random effect is included in the model using the code (1|mountainRange):\n\nmixed.lmer <-  lmer(testScore ~ bodyLength + (1|mountainRange), data = dragons)\nsummary(mixed.lmer)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: testScore ~ bodyLength + (1 | mountainRange)\n   Data: dragons\n\nREML criterion at convergence: 3991.2\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.4815 -0.6513  0.0066  0.6685  2.9583 \n\nRandom effects:\n Groups        Name        Variance Std.Dev.\n mountainRange (Intercept) 339.7    18.43   \n Residual                  223.8    14.96   \nNumber of obs: 480, groups:  mountainRange, 8\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept) 43.70938   17.13489   2.551\nbodyLength   0.03316    0.07865   0.422\n\nCorrelation of Fixed Effects:\n           (Intr)\nbodyLength -0.924\n\n\n\n\n\n\nThere are no p values in the output. For mixed models, it is best to use other ways to determine if bodyLength has an effect on testScore.\n\n\nIn the Fixed effects section of the output, look at the Estimate for bodyLength.\n\n\n               Estimate  Std. Error   t value\n(Intercept) 43.70938024 17.13488692 2.5508998\nbodyLength   0.03316496  0.07864659 0.4216961\n\n\nNotice that the Std. Error suggests the estimate might be 0 for bodyLength. That means that the effect (think of effect as the slope of the fitted line in a scatterplot) might be no different to 0. In other words, there is no effect.\n\n\n\n\nWe could alternatively compare our model to a reduced model that does not contain our fixed effect bodyLength. We compare the two models in a likelihood ratio test using the function anova().\n\nUsing anova() in this way to compare models can be done for other models, not just mixed models.\n\n\nFit a full model and a reduced model:\n\nfull.lmer <- lmer(testScore ~ bodyLength + (1|mountainRange), data = dragons, REML = FALSE)\nreduced.lmer <- lmer(testScore ~ 1 + (1|mountainRange), data = dragons, REML = FALSE)\n\nNotice the reduced model has 1 instead of bodyLength.\n\n\nREML= Explained\n\nIn our previous model we skipped setting REML - we just left it as default (i.e. REML=TRUE). This means the model used the less biased restricted maximum likelihood method to come up with the estimates. You should report estimates from this model.\nHowever, when you compare models you should use REML=FALSE so the model uses maximum likelihood which doesn’t rely on the coefficients of the fixed effects which are different (ie there is no fixed effect in the reduced model).\n\n\nNow compare the full and reduced model using anova():\n\nanova(reduced.lmer, full.lmer) \n\nData: dragons\nModels:\nreduced.lmer: testScore ~ 1 + (1 | mountainRange)\nfull.lmer: testScore ~ bodyLength + (1 | mountainRange)\n             npar    AIC    BIC  logLik deviance  Chisq Df Pr(>Chisq)\nreduced.lmer    3 3999.7 4012.2 -1996.8   3993.7                     \nfull.lmer       4 4001.5 4018.2 -1996.7   3993.5 0.2075  1     0.6488\n\n\nThe p value under Pr(>Chisq) is not significant, there is no difference between the models. This means having bodyLength in the model explains none of the variance in the testScore. bodyLength has no effect.\n\n\n\n\nYou can also assess models using AIC values. The model with the lower AIC value will fit the data better. Models with similar values are no different.\nAIC values were given in the anova() output above or use the AICc() function in the AICcmodavg package:\n\nlibrary(AICcmodavg)\nAICc(reduced.lmer)\n\n[1] 3999.736\n\nAICc(full.lmer)\n\n[1] 4001.562\n\n\nGenerally, if models are within 2 AICc units of each other they are very similar. Within 5 units they are quite similar, over 10 units difference and you can probably be happy with the model with lower AICc. As with p-values though, there is no “hard line” that’s always correct.\n\n\nSo we can conclude that body size has no effect on dragon intelligence and therefore we will be able to train the small ones and not use the big scary ones!\nIf we had not accounted for mountain range in a mixed model we may have came to the wrong conclusion.\n\n\n\n\nThe mixed model we ran for the dragon data…\n\nmixed.lmer <-  lmer(testScore ~ bodyLength + (1|mountainRange), data = dragons)\n\n…was a random intercept model.\nHere’s the graph to accompany that.\n\n\n\n\n\nNotice that the different coloured lines for each mountain range would have different intercepts but the same slope as each other. (The intercept is the point where a line crosses the y axis if it were that long.)\n\nWe could have run this mixed model instead…\n\nmixed.ranslope <- lmer(testScore ~ bodyLength + (1 + bodyLength|mountainRange), data = dragons) \n\nwhich is a random slope model.\nHere’s the corresponding graph\n\n\n\n\n\nNotice the lines for each mountain range have different intercepts AND slopes.\nTherefore, if your exploration of the data or understanding of the study system suggests that the relationship in each group of the random effect (i.e. in each mountainRange) is different, then use a random slope mixed model.\nChallenge Take some time to do an internet search for another definition of the difference between random intercept and random slope mixed models. Write this definition in your own words in your script."
  },
  {
    "objectID": "mixedmodels1.html#barley-yield-example",
    "href": "mixedmodels1.html#barley-yield-example",
    "title": "Mixed Models 1",
    "section": "Barley yield example",
    "text": "Barley yield example\n\nThe effect of variety and type of nitrogen fertiliser fertType on the yield of barley plants, was tested in a field trial. The plants were grown in 20 different areas of the field called area, that were likely to vary in soil and water qualities that affected yield. The data was called barley"
  },
  {
    "objectID": "mixedmodels1.html#caterpillar-example",
    "href": "mixedmodels1.html#caterpillar-example",
    "title": "Mixed Models 1",
    "section": "Caterpillar example",
    "text": "Caterpillar example\n\nWe want to know if development time can predict the weight of caterpillars. There may be genetic influences on weight too. We can only get 10 caterpillars from each female butterfly so use 14 females. For each of the 140 caterpillars, we note down which 1 to 14 female butterfly laid it, the development time devTime in hours and weight wgt in mg and call the dataset butterflies."
  },
  {
    "objectID": "mixedmodels1.html#petri-dish-example",
    "href": "mixedmodels1.html#petri-dish-example",
    "title": "Mixed Models 1",
    "section": "Petri dish example",
    "text": "Petri dish example\n\nThe effect of two bacterial inhibitors inhib on bacterial growth is tested by growing 10 spots of bacteria per 20 petri dishes dish. Bacterial growth seemed to vary among the petri dishes. Data was called bacteria.\n\n# The random effect is `dish` as there are several bacterial growth per dish so they can be grouped according to what dish they are are in which might have an affect on growth.\nmodel <- lmer(growth ~ inhib +(1|dish), data = bacteria)\n\n\n\nAdapted from the Coding Club tutorial Introduction to Linear Mixed Models by Gabriela K Hajduk."
  },
  {
    "objectID": "mixedmodels1.html#generalised-mixed-models",
    "href": "mixedmodels1.html#generalised-mixed-models",
    "title": "Mixed Models 1",
    "section": "Generalised mixed models",
    "text": "Generalised mixed models\nSometimes the response (dependent) variable is not continuous but a categorical variable. Just as you can use alternative generalized models instead of lm such as logistic or poisson, you can use other generalised mixed models.\nFor example, imagine each of our dragons was scored as passing or failing the intelligence test making the response passFail binary. Then you would use a binary logistic mixed model.\n\nChallenge\nDo an internet search to find the function and the code for a binominal mixed model. Adapt the code to run an analysis on this binomial dragon data where the response variable passFail is if the dragon passed (1) or failed (0) the IQ test. The fixed effect is bodyLength and random effect is mountainRange.\nOnce you have run the analysis, interpret the results (you might want to look back at the binomial response section of the linear model lesson) and write out the results in your script along with a suitable graph."
  },
  {
    "objectID": "mixedmodels1.html#nested-design",
    "href": "mixedmodels1.html#nested-design",
    "title": "Mixed Models 1",
    "section": "Nested design",
    "text": "Nested design\nChallenge\nDo an internet search to find an example of a study or experimental set up, that includes a nested design. You have 3 tasks:\n* Type a simple description of the experiment\n* Name the variables that are the response, the fixed effect, the random effects and what is nested in what.\n* Write out an example of R code for a mixed model with this nested design.\nTip - searching for an R tutorial or guide on a nested mixed model will provide both an example and the code."
  },
  {
    "objectID": "mixedmodels1.html#crossed-random-effects",
    "href": "mixedmodels1.html#crossed-random-effects",
    "title": "Mixed Models 1",
    "section": "Crossed random effects",
    "text": "Crossed random effects\nChallenge\nDo an internet search to find an example of a crossed (partial or fully) design. You have 3 tasks:\n* Type a simple description of the experiment\n* Name the variables that are the response, the fixed effect, the random effects and explain why it is crossed.\n* Then write out an example of R code that would run a mixed model with crossed random effects.\n\n\nAdapted from the Coding Club tutorial Introduction to Linear Mixed Models by Gabriela K Hajduk."
  },
  {
    "objectID": "multivariatemethods.html",
    "href": "multivariatemethods.html",
    "title": "Multivariate Methods",
    "section": "",
    "text": "Principal Components Analysis (PCA) is used when there is a large number of continuous variables that define the samples. It makes the large number of variables into a smaller number of derived variables. For example, PCA might be used to:\n\ngroup species according to shape using many different measurements of their bodies\n\nquantify the qualities of habitats using different measures such as plant species cover, tree density, distance from human disturbance, air quality, noise pollution\n\ncompare the chemistry of different rivers based on multiple chemical variables\n\nunderstand hundreds of gene expression measurements\n\nThe new, smaller set of variables (principle components, PCs) created by PCA, can be used in other statistical analyses, but most commonly are plotted on graphs.\nThink of the first principal component (PC1) as a line of best fit in multivariate space. It explains the maximum amount of variation in the data. The amount of variation is given as a %. The second PC (PC2) is fitted at right angles to the first (i.e., orthogonally) such that it explains as much of the remaining variation as possible. Additional PCs, which must be orthogonal to existing PCs, can then be fitted by the same process.\nVisualising this in two dimensions helps to understand the approach:\n\n\n\n\n\nNow imagine fitting those lines in more than three dimensions!\n\n\n\nConsider a plant physiologist attempting to quantify differences in leaf shape between two species of tree. They record total length (leaf + petiole), leaf length, width at the widest point, width half way along the leaf and petiole length from ten leaves of each species. These data are five dimensional (i.e., five measured variables) and we can use PCA to extract two new variables (PCs) that will allow us to visualise the data in fewer dimensions.\n\nIt is highly likely that there are strong relationships between variables in our example data set (e.g., leaf length vs total length). This means that the principal components are likely to explain a fair bit of the variation (imagine fitting a straight line along a sausage-shaped collection of points in multivariate space). If all variables were completely uncorrelated with each other, then PCA is not going to work very well (imagine trying to fit a line of best fit along a ball-shaped collection of points in multivariate space).\n\n\n\n\nYour data should be formatted with variables as columns and observations as rows. Save the leaf shape data set, leafshape.csv, in a file called data in your R project and import into R to see the required format.\n\nLeaf_shape <- read.csv(file = \"data/leafshape.csv\", header = TRUE)\n\nThe first column is a categorical variable that labels the leaves by species (A or B). We need to assign that to a new object (Species) that we can use later for plotting, and make a new data frame (Leaf_data) with just the variables to be analysed by PCA (columns 2-6).\n\nSpecies <- Leaf_shape$Species\nLeaf_data <- Leaf_shape[, 2:6]\n\nThere are a number of functions and packages in R available for conducting PCA, one of the simplest is the princomp() function in base R (packages already comes with R). To run a PCA, we use:\n\nLeaf_PCA <- princomp(Leaf_data)\n\nCalling the plot() function on the princomp output object produces a score plot. This is the ordination of all 20 leaf samples in the new two-dimensional space defined by PC1 and PC2. Here, we can also label the samples by species with the colour argument, and add a legend.\n\nplot(Leaf_PCA$scores, pch = 16, col = as.factor(Species))\nlegend(0, 0.4, c(\"Species A\", \"Species B\"), pch = 16, col = c(\"black\", \"red\"))\n\n\n\n\n\n\nCode Explanation\n\n\nThe square brackets used in Leaf_data <- Leaf_shape[, 2:6] are called indexing and reference the second to sixth column\n\npch is the size of the points\n\ncol is what variable to use to colour points\n\nas.factor() ensures R treats the Species data as a categorical variable\n\nThe arguments in legend specify the position of it on the graph, the labels, size and colour\n\n\n\n\n\nPoints that are close together have similar values for the original variables.\nPCA produces a lot of graphical and numerical output. To interpret the results you need to understand several things:\n1) How much variance is explained by each component. This can be found by passing the PCA object through summary.\n\nsummary(Leaf_PCA)\n\nImportance of components:\n                          Comp.1     Comp.2     Comp.3    Comp.4       Comp.5\nStandard deviation     0.8302248 0.22418865 0.11987329 0.1035367 0.0089705579\nProportion of Variance 0.9013599 0.06572552 0.01879107 0.0140183 0.0001052315\nCumulative Proportion  0.9013599 0.96708539 0.98587647 0.9998948 1.0000000000\n\n\nThe Proportion of Variance in the second row is the variance each PC (Comp.) explains. In this example, PC1 explains 90% of the variation between the two species with PC2 explaining a further 6.6%. Together, those two axes (the ones you plotted) explain 96.7% of the variance (the Cumulative Proportion row). This means that those original data in five dimensions can be placed almost perfectly on this new two-dimensional plane.\nThe variance explained by each PC (component) can also be visualised by a scree plot. The variance explained always declines with the number of the component. In this example, there is not much difference between PC2 and PC3, meaning PC3 does not explain much more of the variance. Therefore, we only need to use PC1 and 2 to visualise the data.\n\nscreeplot(Leaf_PCA, type = \"lines\")\n\n\n\n\n2) How are the original variables related to the principal components?\nThese relationships are stored as numbers and can be obtained by extracting the loadings from the PCA object.\n\nloadings(Leaf_PCA)\n\n\nLoadings:\n               Comp.1 Comp.2 Comp.3 Comp.4 Comp.5\nTotal_length           0.772  0.244         0.582\nPetiole_length         0.458 -0.169  0.647 -0.586\nLeaf_length            0.320  0.428 -0.627 -0.564\nWidth1         -0.949  0.160 -0.215 -0.163       \nWidth2         -0.300 -0.259  0.826  0.400       \n\n               Comp.1 Comp.2 Comp.3 Comp.4 Comp.5\nSS loadings       1.0    1.0    1.0    1.0    1.0\nProportion Var    0.2    0.2    0.2    0.2    0.2\nCumulative Var    0.2    0.4    0.6    0.8    1.0\n\n\nThe loadings are correlations between the principal components and the original variables (Pearson’s r). Values closest to 1 (positive) or -1 (negative) will represent the strongest relationships, with zero being uncorrelated.\nYou can see that PC1 is positively correlated with the two width variables. R doesn’t bother printing very low correlations, so you can also see that PC1 is uncorrelated with the three length variables. Given the two species are split along the x-axis (PC1) in the score plot, we now know that it is leaf widths which cause this separation. We also know that leaves toward the top of the plot are the longest due to the positive correlations between PC2 and the three length variables (but this does not separate the two species on the plot).\nYou can also produce a biplot with the relationships between the original variables and the principal components overlaid on the score plot.\n\nbiplot(Leaf_PCA)\n\n\n\n\nThe original variables (in red) will have a strong relationship with one of the principal components if they are parallel to that component (eg Width 1 and PC1) and longer arrows represent stronger correlations.\n\n\n\n\n\nLinearity. PCA works best when the relationship between variables are approximately linear. In the absence of linearity it is best to transform variables (e.g., log transform) prior to the analysis.\nCorrelation vs covariance matrices. You can run PCA using a covariance matrix, which is appropriate when all variables are measured on the same scale, or a correlation matrix, which is appropriate if variables are measured on very different scales. These will produce different output because using a covariance matrix is affected by differences in the size of variances among the variables. Researchers also commonly standardise variables prior to the analysis if they would like variables that were measured on different scales to have an equal influence on the output.\nChange between these two options with the cor argument in the princomp function.\n\nLeaf_PCA <- princomp(Leaf_data, cor = FALSE) # uses a covariance matrix\nLeaf_PCA2 <- princomp(Leaf_data, cor = TRUE) # uses a correlation matrix\n\nOutliers. Outliers can have big influence on the results of PCA, especially when using a covariance matrix.\n\n\n\n\nWritten. In the results section, it would be typical to state the amount of variation explained by the first two (or more) PCs and the contribution of different variables to those PCs. In this example, you would state that the first principal component explained 90% of the variation in leaf morphology and was most strongly related to leaf width at the widest point.\nVisual. PCA results are best presented visually as a 2-dimensional plot of PCs. It is common to label the points in some way to seek patterns on the plot (like how we labelled leaves by species above).\n\n\nChallenges\nAnalyse these datasets and create graphs to present your findings.\n1) Blue whale genomic data\nThe citation below takes you to a website where you can download a .txt file with 42 measurements of gene expression in female and male whales found in Antartica or Australia.\nAttard, C. R. M. et al. (2012), Data from: Hybridization of Southern Hemisphere blue whale subspecies and a sympatric area off Antarctica: impacts of whaling or climate change?, Dryad, Dataset, https://doi.org/10.5061/dryad.8m0t6\nSave and read the text file into R and run a pca.\n2) Animal skulls\nSkull Base lists skull length, width, height and weight for many species. For each skull record the measurements on an excel spreadsheet to save as a csv file. Format the data so that you can run a PCA.\nYou can choose what skull groups to compare. Some suggestions are:\n\nFelidae versus Canidae or Mustelidae\n\nSuidae, Cervidae and Bovidae within Artiodactyla\n\nRodentia versus Lagomorpha or Soricomorpha\n\nYou could compare dog breeds using categories such as sporting, working, hounds, toy etc.\n\n\n\n\n\nType ?princomp to get the R help for this function.\nAn nice interactive page to help you understand what PCA is doing."
  },
  {
    "objectID": "multivariatemethods.html#example-1",
    "href": "multivariatemethods.html#example-1",
    "title": "Multivariate Methods",
    "section": "Example 1",
    "text": "Example 1\nEach sample in Mühlbauer et al., 2021 is a square of land in a urban area where the presence or absence of many bird species was recorded as well as environmental characteristics of the area such as human activity, tree density, shrub volume, green cover etc.\nIn the figure below each sampled area is a dot (with colours representing different seasons), bird species in black text and environmental characteristics in grey text. The grey arrows show which environmental characteristics correspond with birds in those areas.\n{fig-alt=“a box with many dots and polygons in coloured light blue, orange or green. Most dots are clustered in the bottom right. Grey arrows from the centre of the cluster go left and right labelled with abbreviations of environmental measurements such as sap, water, svol. Black text indicates abbreviations of bird species such as CoF, CoL, PyP. x axis label is”Axis 1 (10%)” and y is “Axis 2 (7%)”.”}"
  },
  {
    "objectID": "multivariatemethods.html#example-2",
    "href": "multivariatemethods.html#example-2",
    "title": "Multivariate Methods",
    "section": "Example 2",
    "text": "Example 2\nGuellaf et al., (2021) collected data from 19 sites on abundance of aquatic insect species and the environmental factors of those sites. A canonical correspondence analysis indicated various relationships such as which species were typically found in areas where the current speed was high.\nRed arrows are water characteristics, blue triangles are insect species and red circles are sites."
  },
  {
    "objectID": "multivariatemethods.html#example-1-1",
    "href": "multivariatemethods.html#example-1-1",
    "title": "Multivariate Methods",
    "section": "Example 1",
    "text": "Example 1\nDing et al., 2022 measured the expression of over 100,000 genes in 6 individuals (named FM, SM, FF, SF, EA and LA) of a wax-producing bug. The dendrogram in (a) below, shows how cluster analysis revealed one of the three technical replicates from the insect EA was not reliable and should be excluded.\nThe graph in (c) below, demonstrates how cluster analysis grouped these 100,000 genes into 19 “modules”. Further analyses could then narrow down what groups of genes were associated with higher wax secretions."
  },
  {
    "objectID": "multivariatemethods.html#example-2-1",
    "href": "multivariatemethods.html#example-2-1",
    "title": "Multivariate Methods",
    "section": "Example 2",
    "text": "Example 2\nSivaprakasam Padmanaban et al., 2022 measured the quantities of many small chemicals called metabolites in popular tree leaves. Cluster analysis separates the old and young leaves in the dendrogram in (b). PCA is also used (a)."
  },
  {
    "objectID": "power.html",
    "href": "power.html",
    "title": "Power Analyses",
    "section": "",
    "text": "Experiment 1 Measures the weights of swans and geese.\nExperiment 2 Measures the weights of swans and robins.\nWhat experiment is most likely to find a significant difference in weight between species?\n\n\nAnswer\n\nExperiment 2 because the difference between swan and robin weight is bigger. The effect size is bigger.\n\n\n\nEffect sizes can be measured. The effect size in a correlation is r. In a t test, it is a number which is given the letter d. \n\n\n\n\n\n\nExperiment 1 - Measures the heights of 10 female and 10 male giraffes.\nExperiment 2 - Measures the heights of 100 female and 100 male giraffes.\nWhat experiment is most likely to find a significant difference in height?\n\n\nAnswer\n\nExperiment 2 because more of the population is sampled.\n\n\n\n\n\nAll student projects in group A compare p values to an significance level of 0.05.\nAll student projects in group B compare p values to an significance level of 0.01.\nWhat group are most likely to find a significant result?\n\n\nAnswer\n\nGroup A because the p values do not have to be as small to be below 0.05 and to reject the null hypothesis.\n\n\n\n\n\n\nA type one error is when you find a significant result but in reality there is no significant effect or difference.\n\nExample: a student project finds there is a difference in maths performance between girls and boys. But in reality this is unlikely to be the case since other researchers have not found this (Li et al., 2018, Lindberg et al., 2010, Reilly et al., 2019). The student would have made a type 1 error.\n\n\nAll student projects in group A compare p values to an alpha level of 0.05.\nAll student projects in group B compare p values to an alpha level of 0.01.\nWhich cohort is most likely to make type 1 errors in their projects?\n\n\nAnswer\n\nGroup A because the higher alpha level means they are more likely to reject the null hypothesis. They are therefore more likely to mistakenly reject a null hypothesis that in reality is true.\n\n\n\n\n\nA type two error is when you mistakenly accept the null hypothesis. You conclude there is no difference or effect when there really is.\n\n\nExample: you measure the weights of male and female gorillas and find no significant difference. In reality males are a lot heavier. You would have made a type two error.\n\n\nWe want to reduce the risk of both type 1 and 2 errors. While it is debatable how to do this, there is a convention established that we use a significance level of 0.05 and 0.8 power.\n\n\n\n\nPower is the chance that a study will detect an effect if one exists.\nA power analysis can tell us how many samples will give us 80% power (80% is 0.8 as a percentage).\nIn other words, you use an alpha level of 0.05, estimate your effect size, then choose a sample size that gives you 0.8 power.\nA power analysis can also be used to determine how high the power of an analysis was that has already been done.\n\n\nIn the future (and already in some disciplines) conventions might change to use, for example, a significance level of 0.01 and 96% power. If you want to know more search for “the replication crisis” on the internet."
  },
  {
    "objectID": "power.html#power-analyses-to-find-out-sample-size",
    "href": "power.html#power-analyses-to-find-out-sample-size",
    "title": "Power Analyses",
    "section": "Power analyses to find out sample size",
    "text": "Power analyses to find out sample size\nYou want to know how many rats you should weigh, to detect an effect of a drug compared to a placebo where the effect size is 0.5, using a significance level of 0.05 and 0.8 power.\nTo do power calculations in pwr, you leave out sample size (n) but enter effect size (d), significance level and power:\n\npwr.t.test(n = NULL, d = 0.5, sig.level = 0.05, power = 0.8)\n\n\n     Two-sample t test power calculation \n\n              n = 63.76561\n              d = 0.5\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\nWe need 128 rats.\n\n\nIf you have a good reason to expect a difference in a particular direction, which is whether the treatment group is higher or lower than the control, you set alternative = “greater” or alternative = “less” - it doesn’t matter which you choose for this pwr.t.test command. If you cannot be sure if the treatment will be higher or lower weight, you set alternative = “two.sided”."
  },
  {
    "objectID": "power.html#other-statistical-tests",
    "href": "power.html#other-statistical-tests",
    "title": "Power Analyses",
    "section": "Other Statistical Tests",
    "text": "Other Statistical Tests\nThe pwr package has a bunch of functions, but they all pretty much work the same way.\n\n\n\nFunction\nDescription\n\n\n\n\npwr.2p.test\ntwo proportions (equal n)\n\n\npwr.2p2n.test\ntwo proportions (unequal n)\n\n\npwr.anova.test\nbalanced one way ANOVA\n\n\npwr.chisq.test\nchi-square test\n\n\npwr.f2.test\ngeneral linear model\n\n\npwr.p.test\nproportion (one sample)\n\n\npwr.r.test\ncorrelation\n\n\npwr.t.test\nt-tests (one sample, 2 sample, paired)\n\n\npwr.t2n.test\nt-test (two samples with unequal n)\n\n\n\n\nChallenge\nThe function pwr.anova.test() performs a power analysis for a balanced anova (balanced is when all the groups have the same sample size). k is the number of groups to be compared and f is the effect size.\nYou are planning a project to measure the pollution concentration in fish from three lakes (three groups). Use an effect size of 0.2, significance level of 0.05 and power of 0.8. How many fish do you need to catch from each lake?\n\n\n\nAnswer\n\n\npwr.anova.test(f=0.2,k=3,power=0.80,sig.level=0.05)\n\n\n     Balanced one-way analysis of variance power calculation \n\n              k = 3\n              n = 81.29603\n              f = 0.2\n      sig.level = 0.05\n          power = 0.8\n\nNOTE: n is number in each group\n\n\nYou need to catch 81 fish from each lake, 243 in total."
  },
  {
    "objectID": "power.html#choosing-an-effect-size-before-the-experiment",
    "href": "power.html#choosing-an-effect-size-before-the-experiment",
    "title": "Power Analyses",
    "section": "Choosing an Effect Size Before the Experiment",
    "text": "Choosing an Effect Size Before the Experiment\nIf you really have nothing else to go on, assume an effect size of 0.5. However, you can normally do better than that, by looking at previous experiments you, or other people, have run.\nKeep in mind that specifying effect size is not a statistical question, it’s an ecological question of what effect size is meaningful for your particular study? For example, do you want to be able to detect a 25% decline in the abundance of a rare animal or would you be happy detecting a 1% decline? For more explanation read the blog post The Effect Size: The Most difficult Step in Calculating Sample Size Estimates."
  },
  {
    "objectID": "power.html#power-analysis-for-estimating-power",
    "href": "power.html#power-analysis-for-estimating-power",
    "title": "Power Analyses",
    "section": "Power Analysis for Estimating Power",
    "text": "Power Analysis for Estimating Power\nImagine this experiment has already taken place. A new treatment was tested on 40 mice (20 in the control group and 20 in the treatment group) and measurements of success taken. The effect size was found to be 0.3.\nChallenge\nUse the function pwr.t.test and calculate what power the t test had. Use a significance level of 0.05.\n\n\nAnswer\n\n\npwr.t.test(n = 20, d = 0.3, sig.level = 0.05, power = NULL)\n\n\n     Two-sample t test power calculation \n\n              n = 20\n              d = 0.3\n      sig.level = 0.05\n          power = 0.1522683\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\nPower was only about 15%. This means that given the effect size, and sample size, we only detected that effect 15% of the time. So, it probably was not worth doing this experiment!\n\n\nHow big would the sample sizes in the experiment above have had to be to achieve 80% power? We can try n = 30:\n\npwr.t.test(n = 30, d = 0.3, sig.level = 0.05, power = NULL)\n\n\n     Two-sample t test power calculation \n\n              n = 30\n              d = 0.3\n      sig.level = 0.05\n          power = 0.2078518\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\nPower improves to around 20%. We need to try higher sample sizes. Instead of manually plugging in different values for n, we could make R run the power analysis for many different sample sizes. This code calculates and plots power for samples sizes from 2 to 200.\n\nnvals <- seq(2, 200, length.out = 200)\npowvals <- sapply(nvals, function(x) pwr.t.test(d = 0.3, n = x, sig.level = 0.05)$power)\nplot(nvals, powvals,\n  xlab = \"sample size\", ylab = \"power\",\n  main = \"Power curve for sample size for difference in proportions\",\n  lwd = 2, col = \"red\", type = \"l\"\n)\nabline(h = 0.8)\n\n\n\n\n\n\nExplanation of code\n\nnvals is an object made to store a sequence seq() of numbers from 2 to 200\npowvals is an object that will store the calculated powers retrieved using $power\nsapply() takes each number x in nvals and uses it in the function pwr.t.test()\nplot() graphs the numbers stored in nvals and powvals against each other\nxlab, ylab and main are the x and y axes labels and main plot title\nlwd is line width, col is line colour and type =1 is a solid line\nabline() draws horizontal (h) line at 0.8\n\n\nNow we can see that a sample size of around 175 for each group would have given enough power.\n\n\nEffect sizes should be reported in results. If a effect size is not given, it can sometimes be calculated. For example, d can be calculated if the means and standard deviations are given."
  },
  {
    "objectID": "spatial.html",
    "href": "spatial.html",
    "title": "Spatial Data",
    "section": "",
    "text": "Spatial statistics is a huge area with many methods. This is a short introduction.\nExamples of various maps created in R and published on R-bloggers."
  },
  {
    "objectID": "spatial.html#first-a-simple-lm-model",
    "href": "spatial.html#first-a-simple-lm-model",
    "title": "Spatial Data",
    "section": "First a simple lm model",
    "text": "First a simple lm model\nWe will use country IQ data from Hassall and Sherratt’s (2011) paper. Use this simplified form of the data. Save in your data folder in your R project.\nRead in the data:\n\nIQData <- read.table(file = \"data/IQData.txt\", header = TRUE)\nhead(IQData)\n\n  Continent      Country IQ  Disease Nutrition Temperature   GDP DistEEA\n1    Africa      Algeria 83  1974.29    439.38       13.14  7100 4397.88\n2    Africa       Angola 68 19078.39   2142.56       18.79  8400 1154.33\n3    Africa        Benin 70 10870.93   1143.10       25.21  1500 2991.75\n4    Africa     Botswana 70 32483.12    532.08        3.90 12800 1915.18\n5    Africa Burkina_Faso 68 15706.29   1405.28       25.61  1200 3526.84\n6    Africa      Burundi 69 18706.93   1439.56       18.92   300  578.49\n  Longitude Latitude\n1     2.630   28.159\n2    17.541  -12.312\n3     2.338    9.628\n4    23.806  -22.185\n5    -1.765   12.265\n6    29.942   -3.336\n\n\nNote that there are countries, their corresponding IQ, possible explanatory variables for IQ and longitude and latitude.\n\nWe can run a lm model to see if nutrition and disease predict the IQ of a country.\n\nmodel1 <- lm(IQ ~ log(Disease) + log(Nutrition), data=IQData)\nsummary(model1)\n\n\nCall:\nlm(formula = IQ ~ log(Disease) + log(Nutrition), data = IQData)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-21.0732  -4.1154  -0.1215   3.9238  24.0464 \n\nCoefficients:\n               Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    128.6854     3.0463  42.244   <2e-16 ***\nlog(Disease)    -6.3655     0.6525  -9.755   <2e-16 ***\nlog(Nutrition)   0.6057     1.0594   0.572    0.568    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.544 on 171 degrees of freedom\nMultiple R-squared:  0.6915,    Adjusted R-squared:  0.6879 \nF-statistic: 191.6 on 2 and 171 DF,  p-value: < 2.2e-16\n\n\n\n\nWhy disease and nutrition is logged\n\nDuring data exploration, plots reveal Disease and Nutrition to be skewed:\n\nhist(IQData$Disease)\n\n\n\nhist(IQData$Nutrition)\n\n\n\n\nTaking the log of the data makes it more normally distributed:\n\nhist(log(IQData$Disease))\n\n\n\nhist(log(IQData$Nutrition))\n\n\n\n\nThis may improve the fit of the model.\n\n\nChallenge\nIdentify the parts of the summary output that tell you what the effect of Disease and Nutrition is on IQ.\n\n\nAnswer\n\nYou could look at the p values under Pr(>|t|). For log(Disease) it is <2e-16 suggesting significance. For log(Nutrition) it is 0.568 suggesting non significance. You may also look at the Estimate and Adjusted R-squared.\n\n\nSo there we have it - IQ is definitely caused by disease! Or is it…? Since this dataset is spatial it is likely to have the problem of autocorrelation!\n\nSpatial autocorrelation is where samples that are geographically closer to each other are more likely to be similar to each other. They are more likely to have similar values if measured."
  },
  {
    "objectID": "spatial.html#assessing-autocorrelation",
    "href": "spatial.html#assessing-autocorrelation",
    "title": "Spatial Data",
    "section": "Assessing Autocorrelation",
    "text": "Assessing Autocorrelation\nSpatial autocorrelation can be measured using Moran’s I (I the letter not 1 the number).\n\n\nCorrelograms\nWe can use the function spline.correlog() in the package ncf to create a correlogram. This tells us if we have autocorrelation.\nInstall and library load the ncf package.\n\ninstall.packages(\"ncf\") \n\n\nlibrary(ncf) \n\n\nMake an object called correlog_object containing information to plot a correlogram.\n\ncorrelog_object <- spline.correlog(IQData$Longitude, IQData$Latitude, residuals(model1), resamp=0, latlon=TRUE)\nplot(correlog_object)\n\n\n\n\n\n\nExplanation of code\n\n\nspine.correlog() is the function\nIQData$Longitude and IQData$Latitude are the numbers the function used to calculate distances between countries\nresiduals(model1) is a list of the residuals from the lm model. This represents the data.\nresamp=0 tells R to do the calculation once. Try increasing this number (for example, resamp = 100) and plotting to see a confidence interval.\nlatlon=TRUE tells R the co-ordinates are latitude and longitude.\nplot(correlog_object) passes the newly created object through the plot() function to make a correlogram graph.\n\n\n\n\n\nInterpreting the correlogram\nOn the x axis, are the distances between countries (calculated from latitude and longitude). Moran’s I values are on the y axis labelled Correlation. There are high values if the data is very similar, low values closer to 0 if it is not.\n\nthere is no spatial autocorrelation if the values of Moran’s I varies no matter what the distance\nthere is spatial autocorrelation if Moran’s I values are higher for countries closer together (i.e. lower distance apart).\n\nUn-oh! Looks like we have spatial autocorrelation! (Moran’s I values are high on the left of the graph.)\n\n\n\nmoran.test\nWe can also establish if there is autocorrelation by running the moran.test() in package spdep. It gives us a p value that suggests if the data is randomly dispersed.\nLoad the package\n\nlibrary(spdep)\n\nFirst we must create a matrix of distances and weight them:\n\nIQnb <- dnearneigh(as.matrix(IQData[9:10]), 0, 10000,longlat=TRUE) \n\nIQlistw <- nb2listw(IQnb) \n\n\n\nExplanation of code\n\n\ndnearneigh() is a function\nas.matrix means our object will be a matrix\nIQData[9:10] specifies columns 9 and 10 where the longitude and latitude values are.\n0, 10000 specify the lower and upper bounds of the distance class of interest (1 – 10000km is nearly global)\nIQlistw is an object we create ready to pass through the moran.test()\nnb2listw is a function that turns our neighbourhood IQnb object into a weighted list\n\n\n\nNow perform the moran test:\n\nresult <- moran.test(residuals(model1), listw=IQlistw) \nresult\n\n\n    Moran I test under randomisation\n\ndata:  residuals(model1)  \nweights: IQlistw    \n\nMoran I statistic standard deviate = 11.706, p-value < 2.2e-16\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n     6.690929e-02     -5.780347e-03      3.855867e-05 \n\n\nThe p value suggests there is a difference in the Moran I’s values as the distances change. Therefore there is autocorrelation."
  },
  {
    "objectID": "spatial.html#mapping-the-data",
    "href": "spatial.html#mapping-the-data",
    "title": "Spatial Data",
    "section": "Mapping the Data",
    "text": "Mapping the Data\nMaps may help us understand why data is autocorrelated.\nChallenge\nInstall and library load the package ggplot2 and ggmap containing the world dataset which is the coordinates of a simple world map. Then pass the world dataset through the map_data function and assign it as an object that you call world.\n\n\nAnswer\n\n\ninstall.packages(\"ggmap\")\n\n\nlibrary(ggmap) \nworld <- map_data(\"world\") \n\n\n\nNow plot world using ggplot.\n\nworldmap <- ggplot(world, aes(x=long, y=lat, group=group)) +\n  geom_path()\nworldmap\n\n\n\n\nWe can plot the residuals of our model1 on top of this simple map of the world, to see where the spatial autocorrelation occurs.\n\nworldmap + \n  geom_point(data = IQData, aes(Longitude, Latitude, color = ifelse(resid(model1) >= 0, \"blue\", \"red\"),\n                                         group = Continent, \n                                         size=abs(resid(model1))/max(resid(model1))*3)) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\nExplanation of code\n\n\naes(Longitude, Latitude, the position of the residuals. Here given as longitude and latititude.\ncolor = tells what colours to make the points. Here positive and negative residuals are blue and red respectively. Uses the ifelse function.\nsize= tells R how big to make the circles. Here we use the proportional size of the residuals relative to the the biggest (max) residual.\n\n\n\nThe large blue circles on the map are where the countries are most similar. There is also negative correlation where the big red circles are.\n\nAnother way to visualise why there is a autocorrelation problem, is to plot IQ and disease according to the continent of the country (similar to Figure 1 from Hassall and Sherratt, 2011):\n\nggplot(IQData, aes(x=log(Disease), y=IQ, group = Continent)) + \ngeom_point(aes(shape=Continent))\n\n\n\n\nYou can see the data clusters by continent."
  },
  {
    "objectID": "spatial.html#models-for-spatial-data",
    "href": "spatial.html#models-for-spatial-data",
    "title": "Spatial Data",
    "section": "Models for Spatial Data",
    "text": "Models for Spatial Data\nHaving established that we need to account for the spatial autocorrelation, we can run a model. However, there are many models to choose from.\n\nIf you need to analyse spatial data you will need to spend time looking into the most appropriate method. For more guidance try Bivand et al. (2013) Applied Spatial Data Analysis in R or Pebesma and Bivand (2022) Spatial Data Science online book\n\n\nAs an example, we will use a GLS (Generalized Least Squares) model to show how spatial data is included in the model code to account for spatial autocorrelation.\n\n\nGLS models\nThe function gls() in the nlme package will run a GLS model.\n\nlibrary(nlme)\n\ngls() uses similar code to lm but the argument correlation= allows the latitude and longitude data to be included. R will automatically create a spatial correlation structure to include in the model using the longitude and latitude values.\n\nmodel2 <- gls(IQ ~ log(Disease) + log(Nutrition), data=IQData, correlation=corExp(form=~Longitude+Latitude)) \nsummary(model2)\n\nIn the correlation argument we have specified the spatial structure of our data to be corExp in the correlation= argument. However, there are many options with corExp, corGaus, corLin, corRatio, and corSpher most commonly used for spatial autocorrelation.\n\nIt is common for there to be many options to choose from in arguments when running analyses. Be prepared to dedicate time to reading about functions and their arguments so you can confidently decide which is best for your data."
  },
  {
    "objectID": "spatial.html#evaluating-models-with-aics",
    "href": "spatial.html#evaluating-models-with-aics",
    "title": "Spatial Data",
    "section": "Evaluating models with AICs",
    "text": "Evaluating models with AICs\nOne way we could choose which spatial structure option to use, is to run many models with the different options and evaluate the fit of the models. We can evaluate them using an AIC value (this can be used on all sorts of models not just gls models).\nRun the models with the different structures corExp, corGaus, corLin, corRatio, and corSpher:\n\nmodel_corExp <- gls(IQ ~ log(Disease) + log(Nutrition), data=IQData, correlation=corExp(form=~Longitude+Latitude)) \nmodel_corGaus <- gls(IQ ~ log(Disease) + log(Nutrition), data=IQData, correlation=corGaus(form=~Longitude+Latitude)) \nmodel_corLin <- gls(IQ ~ log(Disease) + log(Nutrition), data=IQData, correlation=corLin(form=~Longitude+Latitude)) \nmodel_corRatio <- gls(IQ ~ log(Disease) + log(Nutrition), data=IQData, correlation=corRatio(form=~Longitude+Latitude))\nmodel_corSpher <- gls(IQ ~ log(Disease) + log(Nutrition), data=IQData, correlation=corSpher(form=~Longitude+Latitude))\n\nView the AIC values:\n\nAIC(model_corExp, model_corGaus, model_corLin, model_corRatio, model_corSpher)\n\n               df      AIC\nmodel_corExp    5 1019.224\nmodel_corGaus   5 1089.618\nmodel_corLin    5 1031.139\nmodel_corRatio  5 1054.716\nmodel_corSpher  5 1031.139\n\n\nmodel_corExp has the lowest AIC value so it fits the data best.\n\nChallenge\nView the output of the model by passing it through summary().\n\n\n\n\nChallenge\nCompare the results of this model where spatial autocorrelation is accounted for with the original lm model1 where it was not. What are the differences?\n\n\nAnswer\n\n\nsummary(model1)\n\nNutrition is significant when autocorrelation is accounted for. Disease is still significant (even though p value is not as small)."
  },
  {
    "objectID": "multivariatemethods.html#running-in-r",
    "href": "multivariatemethods.html#running-in-r",
    "title": "Multivariate Methods",
    "section": "Running in R",
    "text": "Running in R\nThere is an imaginary nature reserve with different habitats such as open grassland, old woodland, monocultures of trees and mixes of either 2, 5 or 10 species of tree. The abundance of 14 different species of plant are counted in these different habitats.\n\nwood <- read.csv(file = \"data/wooddata.csv\", header = TRUE)\n\nView the data.\n\nTo understand if certain plant species prefer different parts of the nature reserve we can run a correspondence analysis using the function cca in the vegan package.\n\nlibrary(vegan)\n\nLet’s run the analysis but make a common mistake.\n\nca_wood <- cca(wood)\n\nFigure out what’s causing this error and have a go at debugging it.\n\n\n\n\nThe information contained within the cca object can be seen using str().\n\nstr(ca_wood)\n\n\n\nsummary(ca_wood)\n\nIn the summary() output the eigenvalues show the contribution of each CA to explaining the variation in the data in a similar way to a PCA.\nThe species scores shows how much a species would contribute to defining that CA.\nThe site scores show how much each site contributes to the separation in that CA.\n\n\nplot(ca_wood)\n\n\n\n\nPassing a cca object through plot() shows the habitats (sites) at the centroid of all the species that were abundant there.\nThe species are positioned relative to the sites they are abundant in.\nNote what plant species are plotted close to Old Woodland and Open Grassland. View the data and examine the abundances so you understand why we have ended up with these results.\n\n\nThe variables in these simple examples have been measured on the same scale e.g. leaf dimensions in cm and % cover for plant species in the wood. However, sometimes variables in the same dataset have different scales (cm, counts, degrees) and then you have to make sure you scale the data before running multivariate analyses."
  },
  {
    "objectID": "multivariatemethods.html#running-in-r-1",
    "href": "multivariatemethods.html#running-in-r-1",
    "title": "Multivariate Methods",
    "section": "Running in R",
    "text": "Running in R\nSave the metals data set, harbourmetals.csv from Robert et al. (2008), in your data file and write code to read into R.\n\n\n\n\nThese data have the concentrations of seven metals measured from 60 samples, half from the seaweed Padina crassa and half from the Sargassum linearifolium at two sites. The third column is sample labels and the remaining columns are the metal concentrations.\nLike PCA and CA, the cluster analysis runs on the response variables only so we could make a data frame with just the metal concentrations (columns 4-8).\n\nmetalsOnly <- metal[, 4:8]\n\n\nWe need to make a matrix that quantifies the similarity between each pair of samples. Here we will use the Euclidean distance but there are others to choose from.\n\nmetals.sim <- dist(metalsOnly, method = \"euclidean\")\n\n\nWe then use hclust() with an argument that specifies the linkage method (here single).\n\nmetals.cluster <- hclust(metals.sim, method = \"single\")\n\n\nPass this object through plot()\n\nplot(metals.cluster)\n\n\n\n\nInstead of row numbers, the sample labels from the third column Rep of the original data would be more useful.\nGo back to where you created the metalOnly object and add rownames using the code below. Then run the analysis again.\n\nrownames(metalsOnly) <- metal$Rep\n\n\nThese are basic graphs. They would have to be modified to present in reports.\n\n\n\nAdapted from EnvironmentalComputing."
  },
  {
    "objectID": "mixedmodels2.html",
    "href": "mixedmodels2.html",
    "title": "Mixed Models 2",
    "section": "",
    "text": "Some studies might need a generalised mixed model. Others might have random effects that are crossed or nested.\nUnderstanding these concepts will mean you will be aware when they apply to your own studies."
  },
  {
    "objectID": "mixedmodels2.html#dragons-again",
    "href": "mixedmodels2.html#dragons-again",
    "title": "Mixed Models 2",
    "section": "Dragons again",
    "text": "Dragons again\n\nChallenge part 1\nFind the function and the code for a binominal mixed model. Adapt the code to run an analysis on this binomial dragon data where the response variable passFail is if the dragon passed (1) or failed (0) the IQ test. The fixed effect is bodyLength and random effect is mountainRange.\n\n\n\n\nConsider if you need to check for assumptions.\nWhat numbers from the output are you going to use to interpret the results. You might want to remind yourself of the binomial response section of the linear model lesson. Consider comparing full and null models in an anova or comparing AIC values.\n\n\n\n\n\nChallenge part 2\nOnce you have run the analysis and interpreted the results, write out in your script how you would report this in words.\n\nChallenge part 3\nDecide what a suitable graph would be and create one making sure it’s formatted."
  },
  {
    "objectID": "mixedmodels2.html#lizards-eating",
    "href": "mixedmodels2.html#lizards-eating",
    "title": "Mixed Models 2",
    "section": "Lizards eating",
    "text": "Lizards eating\nImagine an experiment where lizards in tanks are observed everyday for 14 days to see if they eat or not (lizards$eat). Some are in a control group with dead prey and some are in a treatment group with live prey. There are males and females.\n\nlizards <- read.csv(file = \"data/lizard.csv\")\nhead(lizards)\n\n  lizard day   group    sex eat\n1      1   1 control female   0\n2      1   2 control female   0\n3      1   3 control female   1\n4      1   4 control female   0\n5      1   5 control female   0\n6      1   6 control female   0\n\n\nIdentify why this needs a mixed model and therefore what the random variable is.\n\n\nAnswer\n\nWe need to control for the differences among individual animals which are repeated measured. In other words the measurements are not independent because some of them come from the same individual lizard. Therefore, lizard is the random variable.\n\n\nChallenge part 1\nRun a model for this and using the coefficients report the data. Remember they will be log odds.\n\n\nEnsure you know what level the model is using as the baseline or reference level. It might be that it is useful to change this.\n\n\n\n\n\nChallenge part 2\nReport the analysis in words as clearly as you can. Include a graph. Can someone else understand the results from what you have written?"
  },
  {
    "objectID": "spatial.html#cleaning-seabirds",
    "href": "spatial.html#cleaning-seabirds",
    "title": "Spatial Data",
    "section": "Cleaning seabirds",
    "text": "Cleaning seabirds\nChallenge\nIt is useful to learn how to download datasets from various sources. Try downloading the seabird data from Kaggle\nIf you get stuck you can use this source -seabird dataset\n\n\n\nThis data needs cleaning for example, you only need the variables lat, lon, species, max_depth.m so make a dataset only containing them.\n\nNext, practice renaming the variable max_depth.m to maxDepth.\n\nNow run an analysis to understand if the data is spatially autocorrelated.\nThen decide if you need to control for spatial autocorrelation when you test the effect of the species on the maxDepth dived.\n\n\nIf you run into any unexpected problems take the time to think about the possible solutions and how to implement them.\n\n\n\nReferences\nHassall C, Sherratt TN (2011) Statistical inference and spatial patterns in correlates of IQ. Intelligence, 39, 303-310.\n\nAdapted from Christopher Hassall’s Introduction to Spatial Statistics and Mike Treglia’s Landscape Analysis and Modeling"
  }
]