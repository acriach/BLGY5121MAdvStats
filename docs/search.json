[
  {
    "objectID": "aiandgraphs.html",
    "href": "aiandgraphs.html",
    "title": "Graphs and AI",
    "section": "",
    "text": "AI can be used to write code to solve problems you have with your data but you need to know what variables and numbers go where and exactly what you want to achieve.\n\n\nTreat AI like a peer - they could be right but they may have misunderstood.\n\n\nYou MUST credit AI software when you use it. Not doing so is a form of plagiarism.\n\n\nYou MUST NOT use AI to write text for your assignments!\n\n\n\nChallenge\nCreate an account with ChatGPT3.5 (freeversion), Google bard or similar AI. Use it in the challenges below to help you write code.\nTry to get the code to run in R. If it fails try to rephrase your prompt to the AI."
  },
  {
    "objectID": "aiandgraphs.html#graphs",
    "href": "aiandgraphs.html#graphs",
    "title": "Graphs and AI",
    "section": "Graphs",
    "text": "Graphs\nLoad the data (they come with R) then recreate the two graphs by finding code on the internet for the basic graph and building on it.\nLook at this Learning R webpage if you need an introduction or reminder on using ggplot to build graphs in R.\n\nChallenge\nUse this data:\n\ndata(mtcars)\n\nRecreate this graph using the variables miles per gallon mpg, weight wt, displacement disp and automatic or manual am.\n\n\n\n\n\n\n\nHints\n\nThis is a scatterplot. The size of the points are weighted by disp. Points are coloured by am.\nThere are many ways to write code to achieve this graph.\nIf something doesn’t work don’t be afraid to look for alternative code.\n\n\nChallenge\nUse the tooth growth dataset:\n\ndata(ToothGrowth)\n\nRecreate this graph using the variable len and dose.\n\n\n\n\n\n\n\nHints\n\nThis is called a jitter plot with errorbars.\nThe raw data is needed for the jitter part but the mean and standard deviation values are needed for the errorbars. Therefore you may need to summarise the data.\nR sometimes thinks dose is a continuous variable not a factor.\n\n\n\nReflect on using AI\nDid the R code always work first time? Did you or the AI correct it?\nDid you ask the AI to address two problems at once or one at a time and what worked?\nCould you have been more specific in your question?\nDid you give the AI the raw data? Is it a good idea to give your data to AI software?\n\n\n\nCiting AI\nSome norms are emerging for correctly crediting AI.\nOne suggestion is to state in the methods section what part of the work you used AI for, what software was used and what you asked it to do.\nFor example you may write “The R code used to run a power analysis on the chaffinch data was written using artificial intelligence software ChatGPT 3.5 (OpenAI, 2023).\nThe reference would be “OpenAi. 2023. ChatGPT (26 Oct version) [Large language model]. https://chat.opanai.com/chat”"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Advanced Statistics",
    "section": "",
    "text": "Cute fuzzy monsters putting rectangular data tables onto a conveyor belt. Along the conveyor belt line are different automated “stations” that update the data, reading “WRANGLE”, “VISUALIZE”, and “MODEL”. A monster at the end of the conveyor belt is carrying away a table that reads “Complete analysis.”"
  },
  {
    "objectID": "lm.html",
    "href": "lm.html",
    "title": "Linear Models",
    "section": "",
    "text": "Linear models can be used to run regressions (where the response and predictor are both continuous) or t-tests and ANOVAs (where the response is continuous and the predictor is a factor.)"
  },
  {
    "objectID": "lm.html#running-the-analysis",
    "href": "lm.html#running-the-analysis",
    "title": "Linear Models",
    "section": "Running the analysis",
    "text": "Running the analysis\nIn R you can fit linear models using the function lm.\n\nlm(loght ~ temp, data = Plant_height)\n\nThe response variable loght goes before the tilde. After the tilde we list the predictor variables, only temp in this case.\nThe data = argument specifies the data frame from which the variables will be taken.\n\nTo obtain detailed output (e.g., coefficient values, R2, test statistics, p-values, confidence intervals etc.), assign the output of the lm function to a new object in R. Then pass that new model object through the summary function.\n\nmodel <- lm(loght ~ temp, data = Plant_height)\nsummary(model)\n\n\nCall:\nlm(formula = loght ~ temp, data = Plant_height)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.97903 -0.42804 -0.00918  0.43200  1.79893 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -0.225665   0.103776  -2.175    0.031 *  \ntemp         0.042414   0.005593   7.583 1.87e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6848 on 176 degrees of freedom\nMultiple R-squared:  0.2463,    Adjusted R-squared:  0.242 \nF-statistic:  57.5 on 1 and 176 DF,  p-value: 1.868e-12"
  },
  {
    "objectID": "lm.html#what-has-lm-just-done",
    "href": "lm.html#what-has-lm-just-done",
    "title": "Linear Models",
    "section": "What has lm just done?",
    "text": "What has lm just done?\nIt has tried to make a line of best fit (the blue line in the graph).\n\n\n\n\n\nThe equation for that line is in the form \\[y = \\alpha + \\beta x \\]\n\\(\\alpha\\) is the intercept (where the line crosses the y axis). \\(\\beta\\) is the slope. (This is the same as the equation for a straight line y = mx + c or y = ax + b you may have encountered before.)\nThe goal of lm is to obtain the best estimates for \\(\\alpha\\) and \\(\\beta\\). \\(\\alpha\\) and \\(\\beta\\) are called the model coefficients.\nTo make it a model rather than just a straight line, it also has an extra bit called the error term \\(\\varepsilon\\). You can think of this as how close the points are to the line. \\(\\varepsilon\\) is not usually reported as part of the equation. \\[y = \\alpha + \\beta x + \\varepsilon \\]"
  },
  {
    "objectID": "lm.html#interpreting-the-results",
    "href": "lm.html#interpreting-the-results",
    "title": "Linear Models",
    "section": "Interpreting the results",
    "text": "Interpreting the results\nThe output given by summary() gives us the \\(\\beta\\) and \\(\\alpha\\) coefficients so we can report the model equation \\[log(plant height) = -0.22566 +0.0421.temperature + \\varepsilon \\]\nLook at the output to find where these numbers came from.\n\nNote that \\(\\beta\\) which is the slope can be interpreted as the amount of change in \\(y\\) for each unit of \\(x\\). For example, as the temperature increases by 1 degree, the log(plant height) increases by 0.0241.\n\n\nPassing the model object through summary() also gives us the t-statistics and p-values related to each predictor. These test the null hypothesis that the true value for the coefficient is 0.\nFor the intercept we usually don’t care if it is zero or not, but for the other coefficient (the slope), a value significantly differing from zero indicates that there is an association between that predictor and the response. In this example, temperature affects plant height.\nWhilst the t-statistics and p-values indicate a significant association, the strength of the association is captured by the R2 value. R2 is the proportion of variance in the response that is explained by the predictor(s).\nThe F-statistic and associated p-value indicates whether the model as a whole is significant. The model will always be significant if any of the coefficients are significant. With only one predictor variable, the probability associated with the t test, that tests whether the slope differs from zero, is identical to the probability associated with the F statistic.\nWe can also obtain 95% confidence intervals for the two parameters. Checking that the intervals for the slope do not include zero is another way of showing that there is an association between the dependent and predictor variable.\n\nconfint(model)\n\n                  2.5 %      97.5 %\n(Intercept) -0.43047074 -0.02085828\ntemp         0.03137508  0.05345215\n\n\nIn summary, you could report\n\nThe model (log(plant height) = -0.22566 + 0.0421.temperature, R^2 = 0.246) was significant (F(1,176) = 57.5, p < 0.001) with temperature significantly predicting (t = 7.583, p < 0.001) the height of the plants. This means that when temperature increases by 1 degree the plant height increases by 0.042 (CI 0.031, 0.053).\n\nIf you have run several analyses (or if there is more than one predictor), it may be useful to present the results as a table with coefficient values, standard errors and p-values for each explanatory variable. What parts you choose to report is down to discipline, style of the journal or what the writer thinks should be emphasised to answer the results question.\n\n\nAssumptions to check\nBut to have confidence in our results we should check the data met the assumptions.\nIndependence. For all the data in these examples we’ll assume the observations are independent of each other.\n\nThere are a variety of measures for dealing with non-independence. These include ensuring all important predictors are in the model; averaging across nested observations; or using a mixed-model (covered in another lesson).\n\n\nLinearity. There is no point trying to fit a straight line to data that are curved!\nPassing model through plot() gives four graphs. The first is a plot of residuals versus fitted values. Curvilinear relationships produce patterns in such plots.\n\nplot(model)\n\nThe absence of strong patterning in the first plot indicates the assumption of linearity is valid.\nClick here to see what patterns of residuals you would expect with curved relationships\n\nConstant variance If the plot of residuals versus fitted values is fan-shaped, the assumption of constant variance (homogeneity of variance) is violated.\n\nNormality. Checks of whether the data are normally distributed are usually performed by either plotting a histogram of the residuals or via a quantile plot where the residuals are plotted against the values expected from a normal distribution (the second of the figures obtained by plot(model)). If the points in the quantile plot lie mostly on the line, the residuals are normally distributed.\n\nhist(model$residuals) # Histogram of residuals\n\n\n\nplot(model, which = 2) # Quantile plot\n\n\n\n\nProblems with variance or normality can be addressed via transformations or by using a Generalised Linear Model, GLM. Note, however, that linear regression is reasonably robust against violations of constant variance and normality."
  },
  {
    "objectID": "lm.html#visualising-data",
    "href": "lm.html#visualising-data",
    "title": "Linear Models",
    "section": "Visualising data",
    "text": "Visualising data\nWe could plot a boxplot or bar chart with overlayed points. An alternative is a violin plot using geom_violin.\n\n  ggplot(aes(x = River_name, y = pH), data = River_pH) +\n  geom_violin()\n\n\n\n\nOverlay the means and their 95% confidence intervals using stat_summary(). Change the axis labels using xlab() and ylab().\n\n  ggplot(aes(x = River_name, y = pH), data = River_pH) +\n  geom_violin() +\n  stat_summary(fun = \"mean\", size = 0.2) +\n  stat_summary(fun.data = \"mean_cl_normal\", geom = \"errorbar\", width = 0.2) + \n  xlab(\"River\") +\n  ylab(\"pH of River\")\n\n\n\n\n\n\nfun and fun.data explained\n\nfun and fun.data are arguments in stat_summary() that do statistical operations to data. fun takes the data and returns a single value such as the mean. fun.data calculates three values for each group: y, ymin and ymax. In our case, ymin is the lower confidence interval and ymax is the upper confidence interval.\n\n\nChallenge\nRead in the Palmer Penguins dataset (penguins.csv). Make a violin plot of body_weight_g for the two groups in sex.\nCan you search the internet to find out how to remove NA values?\n\n\nSolution\n\nRead in the data\n\npenguins <- read.csv(file = \"data/penguins.csv\")\n\nMake a violin plot with mean, error bars, and axes labels.\n\n  ggplot(aes(x = sex, y = body_mass_g), data = penguins) +\n  geom_violin() +\n  stat_summary(fun = \"mean\", size = 0.2) +\n  stat_summary(fun.data = \"mean_cl_normal\", geom = \"errorbar\", width = 0.2) + \n  xlab(\"Penguin Sex\") +\n  ylab(\"Body Mass (g)\")\n\n\n\n\nOne solution (of many) to remove NA values is piping the data into the drop_na() function from the tidyr package. The resulting data can be piped into ggplot.\n\nlibrary(tidyr)\npenguins %>% \n        drop_na(sex) %>%\n        ggplot(aes(x = sex, y = body_mass_g)) +\n  geom_violin() +\n  stat_summary(fun = \"mean\", size = 0.2) +\n  stat_summary(fun.data = \"mean_cl_normal\", geom = \"errorbar\", width = 0.2) + \n  xlab(\"Penguin Sex\") +\n  ylab(\"Body Mass (g)\")"
  },
  {
    "objectID": "lm.html#fitting-a-model",
    "href": "lm.html#fitting-a-model",
    "title": "Linear Models",
    "section": "Fitting a model",
    "text": "Fitting a model\nAs the previous example, use lm() and then put the resulting model through summary().\n\nmodel <- lm(pH ~ River_name, data = River_pH)\nsummary(model)\n\nlm() has used the same equation but since our predictor is a factor/category oppose to numeric, how we interpret the results is different.\nThere are two groups - A and B. One is taken by the model as the baseline (A), the other as the contrast (B). The first level alphabetically is chosen by R as the baseline.\nThe intercept in the output is the estimated mean for the baseline, i.e. for River A. The B estimate is the estimated mean difference in pH between River A and B. We can therefore write the equation for this model as:\n\\[pH = 8.6615 -2.2529 \\times x\\] where \\(x = 1\\) if the river is river B or \\(x = 0\\) if it is the baseline river A.\nWe could report: There is a significant difference in pH between river A (mean = 8.66) and river B (mean = 6.41; t = -6.98, p < 0.001).\n\n\nAre these results the same as running a t test?\n\nYes! Same t and p values.\n\nt.test(pH ~ River_name, data = River_pH, var.equal = TRUE)\n\n\n    Two Sample t-test\n\ndata:  pH by River_name\nt = 6.9788, df = 18, p-value = 1.618e-06\nalternative hypothesis: true difference in means between group A and group B is not equal to 0\n95 percent confidence interval:\n 1.574706 2.931168\nsample estimates:\nmean in group A mean in group B \n       8.661497        6.408560 \n\n\n\n\nChallenge\nRun a model and report if there is an effect of sex on the body_mass_g of penguins.\n\n\nSolution\n\n\nmodel <- lm(body_mass_g ~ sex, data = penguins)\nsummary(model)\n\nThere is a significant effect of sex on penguin body mass with males larger (mean = 4545.68g) than females (mean = 3862.27g; t = 8.54, p < 0.001).\n\n\nChallenge\nCheck the assumptions of the penguin sex model using plot.\nDo you think it meets the assumptions?\n\n\nSolution\n\n\nplot(model)\n\nA linear relationship is not relevant here as the predictor is categorical not numeric. Something is wrong with the normality of the residuals. This would alert us to some other variable effecting the data - in this case penguin species. The variance might be greater in males than females."
  },
  {
    "objectID": "lm.html#running-the-analysis-1",
    "href": "lm.html#running-the-analysis-1",
    "title": "Linear Models",
    "section": "Running the analysis",
    "text": "Running the analysis\nSave the turtle hatching data, Turtles.csv, import into R and check the temperature variable is a factor with the str function.\n\nTurtles <- read.csv(file = \"data/Turtles.csv\", header = TRUE)\nstr(Turtles)\n\n'data.frame':   40 obs. of  2 variables:\n $ Temperature: int  15 15 15 15 15 15 15 15 15 15 ...\n $ Days       : int  37 43 45 54 56 65 62 73 74 75 ...\n\n\nR is treating Temperature as a numeric (int means integer). We need to change that variable to become a factor (categories).\n\nTurtles$Temperature <- factor(Turtles$Temperature)\n\nNow run the model using lm.\n\nturtle_model <- lm(Days ~ Temperature, data = Turtles)\nsummary(turtle_model)\n\n\nCall:\nlm(formula = Days ~ Temperature, data = Turtles)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-28.200  -9.225   1.650   9.025  19.400 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)     58.400      4.092  14.273  < 2e-16 ***\nTemperature20  -13.800      5.787  -2.385   0.0225 *  \nTemperature25   -9.200      5.787  -1.590   0.1206    \nTemperature30  -38.300      5.787  -6.619 1.04e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 12.94 on 36 degrees of freedom\nMultiple R-squared:  0.5711,    Adjusted R-squared:  0.5354 \nF-statistic: 15.98 on 3 and 36 DF,  p-value: 9.082e-07\n\n\n\nIf we thought we needed a post hoc test we could pass our model object through emmeans() from emmeans package.\n\nlibrary(emmeans)\nemmeans(turtle_model, pairwise ~ Temperature)\n\n$emmeans\n Temperature emmean   SE df lower.CL upper.CL\n 15            58.4 4.09 36     50.1     66.7\n 20            44.6 4.09 36     36.3     52.9\n 25            49.2 4.09 36     40.9     57.5\n 30            20.1 4.09 36     11.8     28.4\n\nConfidence level used: 0.95 \n\n$contrasts\n contrast                      estimate   SE df t.ratio p.value\n Temperature15 - Temperature20     13.8 5.79 36   2.385  0.0983\n Temperature15 - Temperature25      9.2 5.79 36   1.590  0.3970\n Temperature15 - Temperature30     38.3 5.79 36   6.619  <.0001\n Temperature20 - Temperature25     -4.6 5.79 36  -0.795  0.8563\n Temperature20 - Temperature30     24.5 5.79 36   4.234  0.0008\n Temperature25 - Temperature30     29.1 5.79 36   5.029  0.0001\n\nP value adjustment: tukey method for comparing a family of 4 estimates"
  },
  {
    "objectID": "lm.html#assumptions-to-check-1",
    "href": "lm.html#assumptions-to-check-1",
    "title": "Linear Models",
    "section": "Assumptions to check",
    "text": "Assumptions to check\n\nplot(turtle_model)\n\n\n\n\n\n\n\n\n\n\n\n\nhist(turtle_model$residuals)\n\n\n\n\nRemember: the first graph produced by plot(), tells us about homogeneity of variance (equal variance). Look for an even spread of the residuals on the y axis for each of the levels on the x axis.\nThe second plot and the histogram from hist() tells us about normality."
  },
  {
    "objectID": "lm.html#interpreting-the-results-1",
    "href": "lm.html#interpreting-the-results-1",
    "title": "Linear Models",
    "section": "Interpreting the results",
    "text": "Interpreting the results\nChallenge\nGiven the output, write out how you could report these results. There will be many ways.\nHint: Look at how we reported the examples before. Look at how a paper in your discipline reported results. Look at how ANOVA is reported.\nChallenge\nRun a lm model to test the effect of penguin species on body_mass_g. Report the results.\n\n\nYou might have previously been taught to run an anova and post hoc Tukey test on continuous data with 3 or more factors. If you run those tests using the code below you get the same result.\nTurtle_aov <- aov(Days ~ Temperature, data = Turtles) \nsummary(Turtle_aov)\nTukeyHSD(Turtle_aov)"
  },
  {
    "objectID": "lm.html#running-the-analysis-2",
    "href": "lm.html#running-the-analysis-2",
    "title": "Linear Models",
    "section": "Running the analysis",
    "text": "Running the analysis\nWe can fit a model to test whether the probability of crab presence changes with time (a factor) and distance (a continuous variable).\nFirst make sure R thinks Time is a factor.\n\ncrabs$Time <- factor(crabs$Time)\n\nThe response variable (presence/absence of crabs) is binomial, so we use family=binomial in the glm.\n\ncrab_glm <- glm(CrabPres ~ Time * Dist, family = \"binomial\", data = crabs)"
  },
  {
    "objectID": "lm.html#assumptions-to-check-2",
    "href": "lm.html#assumptions-to-check-2",
    "title": "Linear Models",
    "section": "Assumptions to check",
    "text": "Assumptions to check\nAssumption - There is a straight line relationship between the logit function of the mean of \\(y\\) and the predictors \\(x\\)\nFor this assumption, we check the residual plot for non-linearity, or a U-shape.\n\nplot(crab_glm, which = 1)\n\n\n\n\nUnfortunately, passing the glm object through the plot function gives us a very odd looking plot due to the discreteness of the data (i.e., many points on top of each other).\nFor a more useful plot we can instead fit the model using the manyglm() function in the mvabund package.\n\n\n\n\nlibrary(mvabund)\ncrab_manyglm <- manyglm(CrabPres ~ Time * Dist, family = \"binomial\", data = crabs)\nplot(crab_manyglm)\n\n\n\n\nIn our case there is no evidence of non-linearity.\nIf the residuals seem to go down then up, or up then down, we may need to add a polynomial function of the predictors using the poly function."
  },
  {
    "objectID": "lm.html#interpreting-the-results-2",
    "href": "lm.html#interpreting-the-results-2",
    "title": "Linear Models",
    "section": "Interpreting the results",
    "text": "Interpreting the results\nFor binomial models in particular the p-values from the summary function are not reliable, and we prefer to use the anova function to see if predictors are significant.\n\nsummary(crab_glm)\n\n\nCall:\nglm(formula = CrabPres ~ Time * Dist, family = \"binomial\", data = crabs)\n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)  \n(Intercept) -1.71431    0.68664  -2.497   0.0125 *\nTime10       1.29173    0.87194   1.481   0.1385  \nDist         0.02522    0.11137   0.226   0.8208  \nTime10:Dist  0.05715    0.14149   0.404   0.6863  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 71.097  on 56  degrees of freedom\nResidual deviance: 63.466  on 53  degrees of freedom\nAIC: 71.466\n\nNumber of Fisher Scoring iterations: 4\n\nanova(crab_glm, test = \"Chisq\")\n\nAnalysis of Deviance Table\n\nModel: binomial, link: logit\n\nResponse: CrabPres\n\nTerms added sequentially (first to last)\n\n          Df Deviance Resid. Df Resid. Dev Pr(>Chi)   \nNULL                         56     71.097            \nTime       1   6.6701        55     64.427 0.009804 **\nDist       1   0.7955        54     63.631 0.372448   \nTime:Dist  1   0.1647        53     63.466 0.684852   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe p-value for Time is P<0.01 so we conclude there is an effect of time on the presence of crabs, but no effect of distance or interaction between time and distance.\n\n\nanova results\n\nWhen there is more than one predictor, the maths ANOVA uses can be done in three different ways. These ways are named type I, II and III. This R bloggers article explains the differences.\nOur crab example is approximately balanced (even sample numbers in each group) so whatever version of ANOVA R uses we’ll get the same results. However, if you have unbalanced data you could compare differences among type I, II and III ANOVAs using the function Anova() in the car package.\n\nanova(lm_model) # default is type 1\ncar::Anova(lm_model, type = 2)\ncar::Anova(lm_model, type = 3)\n\n\n\n\nThis sample is reasonably large, so these p-values should be a good approximation. For a small sample it is often better to use resampling to calculate p-values. When you use manyglm the summary and anova functions use resampling by default.\nIn this case the results are quite similar, but in small samples it can often make a big difference.\n\n\n\n\nOptimising the model\n\nWhen there is more than one predictor you can try reducing the model by removing predictors and comparing models. We can use a number called the AIC to compare. Lower AICs are better.\n\nstep(crab_glm, test = \"Chi\")\n\nStart:  AIC=71.47\nCrabPres ~ Time * Dist\n\n            Df Deviance    AIC     LRT Pr(>Chi)\n- Time:Dist  1   63.631 69.631 0.16472   0.6849\n<none>           63.466 71.466                 \n\nStep:  AIC=69.63\nCrabPres ~ Time + Dist\n\n       Df Deviance    AIC    LRT Pr(>Chi)   \n- Dist  1   64.427 68.427 0.7955  0.37245   \n<none>      63.631 69.631                   \n- Time  1   70.275 74.275 6.6438  0.00995 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nStep:  AIC=68.43\nCrabPres ~ Time\n\n       Df Deviance    AIC    LRT Pr(>Chi)   \n<none>      64.427 68.427                   \n- Time  1   71.097 73.097 6.6701 0.009804 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nCall:  glm(formula = CrabPres ~ Time, family = \"binomial\", data = crabs)\n\nCoefficients:\n(Intercept)       Time10  \n     -1.609        1.535  \n\nDegrees of Freedom: 56 Total (i.e. Null);  55 Residual\nNull Deviance:      71.1 \nResidual Deviance: 64.43    AIC: 68.43\n\n\nstep() removes the interaction (Dist * Time), then Dist and the AIC improves (gets lower). This confirms they are not predictors of the response."
  },
  {
    "objectID": "lm.html#communicating-the-results",
    "href": "lm.html#communicating-the-results",
    "title": "Linear Models",
    "section": "Communicating the results",
    "text": "Communicating the results\nYou can use the p values to report results like in other tests, e.g., “There is strong evidence that the presence of crabs varies with time (p = 0.01).” For multiple predictors it’s best to display the results in a table.\n\n\nTip: People get stuck interpreting binomial results because they do not have a clear idea of what the baseline (reference) groups are in their models. In this example we would ensure we know that baseline for the response CrabPres is absence of crabs and baseline for Time is time point 5.\n\n\nThe coefficients for the intercept is the value of the response variable (on a logit scale) when the factor predictors (Time in our example) is the baseline (time point 5 in our example) and the numeric predictors (Dist) is 0. The coefficient for Time (a factor) tells us the difference in the response between the baseline and the other group of the factor (the difference between time point 5 and time point 10).\nThe coefficients for numeric predictors can show negative or positive relationships with the response.\nThe coefficient numbers (called log odds) are difficult for you (and your readers) to interpret. Many people convert them into effect sizes called odds ratios to report them.\n\nexp(coef(crab_glm)) # calculates the exponential of the coefficients in the model i.e. turns log odds into odds ratios\n\n(Intercept)      Time10        Dist Time10:Dist \n  0.1800871   3.6390656   1.0255455   1.0588183 \n\n\nOdds ratios above 1 mean crabs are more likely to be present (present is coded as 1 in the response CrabPres). Odds ratios below 1 mean crabs are less likely to be present.\nThe odds ratio for Time is 3.6. We report “Crabs are 3.6 times more likely to be present at time point 10 compared to time point 5”.\n\nTip if the odds ratio is below 1 try recoding the explanatory variables so that another group is the baseline.\n\n\nFor numeric predictors a positive odds ratio such as 3.21 would mean that a 1 unit increase in the predictor, increases the odds of the response being present by 3.21. However, our odds ratio for distance is negative which is more difficult to put into words and relate back to the research question. One solution is to express it as the % decrease. For example, (0.97–1) * 100 = -3%. Then we can write “Each additional increase of one in distance is associated with an 3% decrease in the odds of a crab being present.\nChallenge\nWhat plots do you think could be used to present this data?"
  },
  {
    "objectID": "lm.html#running-the-analysis-3",
    "href": "lm.html#running-the-analysis-3",
    "title": "Linear Models",
    "section": "Running the analysis",
    "text": "Running the analysis\n\nThis example has counts of different animal groups at control sites and sites where bush regeneration has been carried out (treatment). We will use only one group of animals - slugs (Soleolifera is the order name of terrestrial slugs) to see if the the bush regeneration activities have affected slug abundance.\nSave revegetation.csv and import into R and view the data.\n\nreveg <- read.csv(\"data/revegetation.csv\", header = T)\n\nIf you view the frequency histogram of the slug counts, you will see that it is very skewed, with many small values and few large counts.\n\nhist(reveg$Soleolifera)\n\n\n\n\n\nThe default distribution for count data is the Poisson. The Poisson distribution assumes the variance equals the mean. This is quite a restrictive assumption which ecological count data often violates. We may need to use the more flexible negative-binomial distribution instead.\n\n\nWe can use a GLM to test whether the counts of slugs (from the order Soleolifera) differ between control and regenerated sites. To fit the GLM, we will use the manyglm function instead of glm so we have access to more useful residual plots.\nTo fit the GLM, load the mvabund package then fit the following model:\n\nlibrary(mvabund)\nslug_glm <- manyglm(Soleolifera ~ Treatment, family = \"poisson\", data = reveg)\n\nTreatment is the predictor variable with two levels, control and revegetated."
  },
  {
    "objectID": "lm.html#assumptions-to-check-3",
    "href": "lm.html#assumptions-to-check-3",
    "title": "Linear Models",
    "section": "Assumptions to check",
    "text": "Assumptions to check\nBefore looking at the results, look at the residual plot to check the assumptions.\n\nplot(slug_glm)\n\n\n\n\nIt’s hard to say whether there is any non-linearity in this plot, this is because the predictor is binary (control vs revegetated).\nLooking at the mean-variance assumption, it does appear as though there is a fan shape. The residuals are more spread out on the right than the left - we call this overdispersion.\nThis tells us the mean-variance assumption of the Poisson is probably violated. We should try a different distribution. We can instead fit a negative-binomial distribution in manyglm by changing the family argument to family=\"negative binomial\".\n\nslug_glm2 <- manyglm(Soleolifera ~ Treatment, family = \"negative binomial\", data = reveg)\n\nLook again at the residual plot:\n\nplot(slug_glm2)\n\n\n\n\nThis seems to have improved the residual plot. There is no longer a strong fan shape, so we can go ahead and look at the results."
  },
  {
    "objectID": "lm.html#interpreting-the-results-3",
    "href": "lm.html#interpreting-the-results-3",
    "title": "Linear Models",
    "section": "Interpreting the results",
    "text": "Interpreting the results\nWe can use summary and anova.\n\nanova(slug_glm2)\n\nTime elapsed: 0 hr 0 min 0 sec\n\n\nAnalysis of Deviance Table\n\nModel: Soleolifera ~ Treatment\n\nMultivariate test:\n            Res.Df Df.diff   Dev Pr(>Dev)   \n(Intercept)     48                          \nTreatment       47       1 10.52    0.004 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nArguments: P-value calculated using 999 iterations via PIT-trap resampling.\n\nsummary(slug_glm2)\n\n\nTest statistics:\n                     wald value Pr(>wald)    \n(Intercept)               1.502     0.030 *  \nTreatmentRevegetated      3.307     0.001 ***\n--- \nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nTest statistic:  3.307, p-value: 0.001 \nArguments: P-value calculated using 999 resampling iterations via pit.trap resampling.\n\n\nBoth tests indicate treatment has an effect (p<0.01)."
  },
  {
    "objectID": "lm.html#communicating-the-results-1",
    "href": "lm.html#communicating-the-results-1",
    "title": "Linear Models",
    "section": "Communicating the results",
    "text": "Communicating the results\nYou could write “There is strong evidence of a positive effect of bush regeneration on the abundance of slugs from the order Soleolifera (p < 0.01)”. For multiple predictors it’s best to display the results in a table.\nYou should also indicate which distribution was used (e.g. negative-binomial) and if resampling was used. “We used a negative-binomial generalised linear model due to overdispersion evident in the data. Bootstrap resampling was used with 1000 resamples” (1000 is the default when using manyglm()).\nChallenge\nWhat graph could be used to visualise the differences in slug counts between control and revegetated sites.\n\n\nSolution\n\nThere are various solutions. Boxplot is one.\n\nboxplot(Soleolifera ~ Treatment, ylab = \"Count\", xlab = \"Treatment\", data = reveg)\n\n\n\n\n\n\n\nAdapted from EnvironmentalComputing and Herman et al., 2021 Statistical Analysis for Public Health: Simple linear regression"
  },
  {
    "objectID": "multivariatemethods.html",
    "href": "multivariatemethods.html",
    "title": "Multivariate Methods",
    "section": "",
    "text": "Principal Components Analysis (PCA) is used when there is a large number of continuous variables that define the samples. It makes the large number of variables into a smaller number of derived variables. For example, PCA might be used to:\n\ngroup species according to shape using many different measurements of their bodies\n\nquantify the qualities of habitats using different measures such as plant species cover, tree density, distance from human disturbance, air quality, noise pollution\n\ncompare the chemistry of different rivers based on multiple chemical variables\n\nunderstand hundreds of gene expression measurements\n\nThe new, smaller set of variables (principle components, PCs) created by PCA, can be used in other statistical analyses, but most commonly are plotted on graphs.\nThink of the first principal component (PC1) as a line of best fit in multivariate space. It explains the maximum amount of variation in the data. The amount of variation is given as a %. The second PC (PC2) is fitted at right angles to the first (i.e., orthogonally) such that it explains as much of the remaining variation as possible. Additional PCs, which must be orthogonal to existing PCs, can then be fitted by the same process.\nVisualising this in two dimensions helps to understand the approach:\n\n\n\n\n\nNow imagine fitting those lines in more than three dimensions!\n\n\n\nConsider a plant physiologist attempting to quantify differences in leaf shape between two species of tree. They record total length (leaf + petiole), leaf length, width at the widest point, width half way along the leaf and petiole length from ten leaves of each species. These data are five dimensional (i.e., five measured variables) and we can use PCA to extract two new variables (PCs) that will allow us to visualise the data in fewer dimensions.\n\n\n\nalt.text=A grenn leaf with double ended arrows indicating the total length, leaf length, width at the widest point, width half way along the leaf and petiole length.\n\n\nIt is highly likely that there are strong relationships between variables in our example data set (e.g., leaf length vs total length). This means that the principal components are likely to explain a fair bit of the variation (imagine fitting a straight line along a sausage-shaped collection of points in multivariate space). If all variables were completely uncorrelated with each other, then PCA is not going to work very well (imagine trying to fit a line of best fit along a ball-shaped collection of points in multivariate space).\n\n\n\n\nYour data should be formatted with variables as columns and observations as rows. Save the leaf shape data set, leafshape.csv, in a file called data in your R project and import into R to see the required format.\n\nLeaf_shape <- read.csv(file = \"data/leafshape.csv\", header = TRUE)\n\nThe first column is a categorical variable that labels the leaves by species (A or B). We need to assign that to a new object (Species) that we can use later for plotting, and make a new data frame (Leaf_data) with just the variables to be analysed by PCA (columns 2-6).\n\nSpecies <- Leaf_shape$Species\nLeaf_data <- Leaf_shape[, 2:6]\n\nThere are a number of functions and packages in R available for conducting PCA, one of the simplest is the princomp() function in base R (packages already comes with R). To run a PCA, we use:\n\nLeaf_PCA <- princomp(Leaf_data)\n\nCalling the plot() function on the princomp output object produces a score plot. This is the ordination of all 20 leaf samples in the new two-dimensional space defined by PC1 and PC2. Here, we can also label the samples by species with the colour argument, and add a legend.\n\nplot(Leaf_PCA$scores, pch = 16, col = as.factor(Species))\nlegend(0, 0.4, c(\"Species A\", \"Species B\"), pch = 16, col = c(\"black\", \"red\"))\n\n\n\n\n\n\nCode Explanation\n\n\nThe square brackets used in Leaf_data <- Leaf_shape[, 2:6] are called indexing and reference the second to sixth column\n\npch is the size of the points\n\ncol is what variable to use to colour points\n\nas.factor() ensures R treats the Species data as a categorical variable\n\nThe arguments in legend specify the position of it on the graph, the labels, size and colour\n\n\n\n\n\nPoints that are close together have similar values for the original variables.\nPCA produces a lot of graphical and numerical output. To interpret the results you need to understand several things:\n1) How much variance is explained by each component. This can be found by passing the PCA object through summary.\n\nsummary(Leaf_PCA)\n\nImportance of components:\n                          Comp.1     Comp.2     Comp.3    Comp.4       Comp.5\nStandard deviation     0.8302248 0.22418865 0.11987329 0.1035367 0.0089705579\nProportion of Variance 0.9013599 0.06572552 0.01879107 0.0140183 0.0001052315\nCumulative Proportion  0.9013599 0.96708539 0.98587647 0.9998948 1.0000000000\n\n\nThe Proportion of Variance in the second row is the variance each PC (Comp.) explains. In this example, PC1 explains 90% of the variation between the two species with PC2 explaining a further 6.6%. Together, those two axes (the ones you plotted) explain 96.7% of the variance (the Cumulative Proportion row). This means that those original data in five dimensions can be placed almost perfectly on this new two-dimensional plane.\nThe variance explained by each PC (component) can also be visualised by a scree plot. The variance explained always declines with the number of the component. In this example, there is not much difference between PC2 and PC3, meaning PC3 does not explain much more of the variance. Therefore, we only need to use PC1 and 2 to visualise the data.\n\nscreeplot(Leaf_PCA, type = \"lines\")\n\n\n\n\n2) How are the original variables related to the principal components?\nThese relationships are stored as numbers and can be obtained by extracting the loadings from the PCA object.\n\nloadings(Leaf_PCA)\n\n\nLoadings:\n               Comp.1 Comp.2 Comp.3 Comp.4 Comp.5\nTotal_length           0.772  0.244         0.582\nPetiole_length         0.458 -0.169  0.647 -0.586\nLeaf_length            0.320  0.428 -0.627 -0.564\nWidth1         -0.949  0.160 -0.215 -0.163       \nWidth2         -0.300 -0.259  0.826  0.400       \n\n               Comp.1 Comp.2 Comp.3 Comp.4 Comp.5\nSS loadings       1.0    1.0    1.0    1.0    1.0\nProportion Var    0.2    0.2    0.2    0.2    0.2\nCumulative Var    0.2    0.4    0.6    0.8    1.0\n\n\nThe loadings are correlations between the principal components and the original variables (Pearson’s r). Values closest to 1 (positive) or -1 (negative) will represent the strongest relationships, with zero being uncorrelated.\nYou can see that PC1 is positively correlated with the two width variables. R doesn’t bother printing very low correlations, so you can also see that PC1 is uncorrelated with the three length variables. Given the two species are split along the x-axis (PC1) in the score plot, we now know that it is leaf widths which cause this separation. We also know that leaves toward the top of the plot are the longest due to the positive correlations between PC2 and the three length variables (but this does not separate the two species on the plot).\nYou can also produce a biplot with the relationships between the original variables and the principal components overlaid on the score plot.\n\nbiplot(Leaf_PCA)\n\n\n\n\nThe original variables (in red) will have a strong relationship with one of the principal components if they are parallel to that component (eg Width 1 and PC1) and longer arrows represent stronger correlations.\n\n\n\n\n\nLinearity. PCA works best when the relationship between variables are approximately linear. In the absence of linearity it is best to transform variables (e.g., log transform) prior to the analysis.\nCorrelation vs covariance matrices. You can run PCA using a covariance matrix, which is appropriate when all variables are measured on the same scale, or a correlation matrix, which is appropriate if variables are measured on very different scales. These will produce different output because using a covariance matrix is affected by differences in the size of variances among the variables. Researchers also commonly standardise variables prior to the analysis if they would like variables that were measured on different scales to have an equal influence on the output.\nChange between these two options with the cor argument in the princomp function.\n\nLeaf_PCA <- princomp(Leaf_data, cor = FALSE) # uses a covariance matrix\nLeaf_PCA2 <- princomp(Leaf_data, cor = TRUE) # uses a correlation matrix\n\nOutliers. Outliers can have big influence on the results of PCA, especially when using a covariance matrix.\n\n\n\n\nWritten. In the results section, it would be typical to state the amount of variation explained by the first two (or more) PCs and the contribution of different variables to those PCs. In this example, you would state that the first principal component explained 90% of the variation in leaf morphology and was most strongly related to leaf width at the widest point.\nVisual. PCA results are best presented visually as a 2-dimensional plot of PCs. It is common to label the points in some way to seek patterns on the plot (like how we labelled leaves by species above).\n\n\nChallenge\nChoose dataset 1) or 2)\n1) Blue whale genomic data\nThe citation below takes you to a website where you can download a .txt file with 42 measurements of gene expression in female and male whales found in Antartica or Australia.\nAttard, C. R. M. et al. (2012), Data from: Hybridization of Southern Hemisphere blue whale subspecies and a sympatric area off Antarctica: impacts of whaling or climate change?, Dryad, Dataset, https://doi.org/10.5061/dryad.8m0t6\nSave and read the text file into R and run a pca.\n2) Animal skulls\nSkull Base lists skull length, width, height and weight for many species. For each skull record the measurements on an excel spreadsheet to save as a csv file. Format the data so that you can run a PCA.\nYou can choose what skull groups to compare. Some suggestions are:\n\nFelidae versus Canidae or Mustelidae\n\nSuidae, Cervidae and Bovidae within Artiodactyla\n\nRodentia versus Lagomorpha or Soricomorpha\n\nYou could compare dog breeds using categories such as sporting, working, hounds, toy etc.\n\n\n\n\n\nType ?princomp to get the R help for this function.\nAn nice interactive page to help you understand what PCA is doing."
  },
  {
    "objectID": "multivariatemethods.html#example-1",
    "href": "multivariatemethods.html#example-1",
    "title": "Multivariate Methods",
    "section": "Example 1",
    "text": "Example 1\nEach sample in Mühlbauer et al., 2021 is a square of land in a urban area where the presence or absence of many bird species was recorded as well as environmental characteristics of the area such as human activity, tree density, shrub volume, green cover etc.\nIn the figure below each sampled area is a dot (with colours representing different seasons), bird species in black text and environmental characteristics in grey text. The grey arrows show which environmental characteristics correspond with birds in those areas.\n\n\n\nalt.text=a box with many dots and polygons in coloured light blue, orange or green. Most dots are clustered in the bottom right. Grey arrows from the centre of the cluster go left and right labelled with abbreviations of environmental measurements such as sap, water, svol. Black text indicates abbreviations of bird species such as CoF, CoL, PyP. x axis label is “Axis 1 (10%)” and y is “Axis 2 (7%)”."
  },
  {
    "objectID": "multivariatemethods.html#example-2",
    "href": "multivariatemethods.html#example-2",
    "title": "Multivariate Methods",
    "section": "Example 2",
    "text": "Example 2\nGuellaf et al., (2021) collected data from 19 sites on abundance of aquatic insect species and the environmental factors of those sites. A canonical correspondence analysis indicated various relationships such as which species were typically found in areas where the current speed was high.\nRed arrows are water characteristics, blue triangles are insect species and red circles are sites.\n\n\n\nalt.text=two boxes divided into quadrants. The top box has red arrows radiating out from a centre point labbelled with abbrviations of physical water qualities. There is a scatter of blue triangles with abbrviations representing insect species. The bottom quadrant box has the same red arrows but a scatter of red circles labelled with the abbreviations of sites."
  },
  {
    "objectID": "multivariatemethods.html#example-1-1",
    "href": "multivariatemethods.html#example-1-1",
    "title": "Multivariate Methods",
    "section": "Example 1",
    "text": "Example 1\nDing et al., 2022 measured the expression of over 100,000 genes in 6 individuals (named FM, SM, FF, SF, EA and LA) of a wax-producing bug. The dendrogram in (a) below, shows how cluster analysis revealed one of the three technical replicates from the insect EA was not reliable and should be excluded.\nThe graph in (c) below, demonstrates how cluster analysis grouped these 100,000 genes into 19 “modules”. Further analyses could then narrow down what groups of genes were associated with higher wax secretions.\n\n\n\nalt.text=four images from a paper. The top diagram is a dendrogram ending in the names of samples such as FF2, FF1, FF3, FM3, FM2 etc. A table on the right is alligned to show the sex and amount of wax secreted by the samples. The middle graphs are not relevant to cluster analysis. The bottom graph is a dendrogram ending in over 100,000 lines. A wide horizontal bar underneath is divided into 19 repeating colours corresponding to 19 groups of genes."
  },
  {
    "objectID": "multivariatemethods.html#example-2-1",
    "href": "multivariatemethods.html#example-2-1",
    "title": "Multivariate Methods",
    "section": "Example 2",
    "text": "Example 2\nSivaprakasam Padmanaban et al., 2022 measured the quantities of many small chemicals called metabolites in popular tree leaves. Cluster analysis separates the old and young leaves in the dendrogram in (b). PCA is also used (a).\n\n\n\nalt.text=three images from a paper. The first is a circle within a square with a scatterplot of pink dots on teh right and black dots on teh left. The second is a dedrogram separating black and pink samples with a heat mat in green and red below. The third is a venn diagram with 4 main areas and numbers in the overlapping areas.\n\n\n\n\nAdapted from EnvironmentalComputing."
  },
  {
    "objectID": "power.html",
    "href": "power.html",
    "title": "Power Analyses",
    "section": "",
    "text": "alt.text=Two geese swimming on blue water with a swan swimming in the background.\n\n\n\n\n\nalt.text=A robin sitting on the post of a fence with snow on the fence and surrounding foliage\n\n\nExperiment 1 Measures the weights of swans and geese.\nExperiment 2 Measures the weights of swans and robins.\nWhat experiment is most likely to find a significant difference in weight between species?\n\n\nAnswer\n\nExperiment 2 because the difference between swan and robin weight is bigger. The effect size is bigger.\n\n\n\nEffect sizes can be measured. The effect size in a correlation is r. In a t test, it is a number which is given the letter d. \n\n\n\n\n\n\n\n\nalt.text=A giraffe striding across a grassy landscape.\n\n\nExperiment 1 - Measures the heights of 10 female and 10 male giraffes.\nExperiment 2 - Measures the heights of 100 female and 100 male giraffes.\nWhat experiment is most likely to find a significant difference in height?\n\n\nAnswer\n\nExperiment 2 because more of the population is sampled.\n\n\n\n\n\nAll student projects in group A compare p values to an significance level of 0.05.\nAll student projects in group B compare p values to an significance level of 0.01.\nWhat group are most likely to find a significant result?\n\n\nAnswer\n\nGroup A because the p values do not have to be as small to be below 0.05 and to reject the null hypothesis.\n\n\n\n\n\n\n\n\nalt.text=Table of error types. If null hypothesis is true and the decision about null hypothesis is don’t reject, then this is the correct inference (true negative) (probability = 1 - alpha). If null hypothesis is true and the decision about the null hypothesis is reject, this is a type 1 error (false positive) (probability = alpha). If null hypothesis is false and the decision about null hypothesis is don’t reject, then this is a type two error (false negative) (probability = beta). If null hypothesis is false and the decision about the null hypothesis is reject, this is the correct inference (true positive) (probability = 1 - beta).\n\n\nA type one error is when you find a significant result but in reality there is no significant effect or difference.\n\nExample: a student project finds there is a difference in maths performance between girls and boys. But in reality this is unlikely to be the case since other researchers have not found this (Li et al., 2018, Lindberg et al., 2010, Reilly et al., 2019). The student would have made a type 1 error.\n\n\nAll student projects in group A compare p values to an alpha level of 0.05.\nAll student projects in group B compare p values to an alpha level of 0.01.\nWhich cohort is most likely to make type 1 errors in their projects?\n\n\nAnswer\n\nGroup A because the higher alpha level means they are more likely to reject the null hypothesis. They are therefore more likely to mistakenly reject a null hypothesis that in reality is true.\n\n\n\n\n\nA type two error is when you mistakenly accept the null hypothesis. You conclude there is no difference or effect when there really is.\n\n\n\nalt.text=A big silverback gorilla sitting in the foreground with a smaller gorilla walking behind.\n\n\n\nExample: you measure the weights of male and female gorillas and find no significant difference. In reality males are a lot heavier. You would have made a type two error.\n\n\nWe want to reduce the risk of both type 1 and 2 errors. While it is debatable how to do this, there is a convention established that we use a significance level of 0.05 and 0.8 power.\n\n\n\n\nPower is the chance that a study will detect an effect if one exists.\nA power analysis can tell us how many samples will give us 80% power (80% is 0.8 as a percentage).\nIn other words, you use an alpha level of 0.05, estimate your effect size, then choose a sample size that gives you 0.8 power.\nA power analysis can also be used to determine how high the power of an analysis was that has already been done.\n\n\nIn the future (and already in some disciplines) conventions might change to use, for example, a significance level of 0.01 and 96% power. If you want to know more search for “the replication crisis” on the internet."
  },
  {
    "objectID": "power.html#power-analyses-to-find-out-sample-size",
    "href": "power.html#power-analyses-to-find-out-sample-size",
    "title": "Power Analyses",
    "section": "Power analyses to find out sample size",
    "text": "Power analyses to find out sample size\nYou want to know how many rats you should weigh, to detect an effect of a drug compared to a placebo where the effect size is 0.5, using a significance level of 0.05 and 0.8 power.\nTo do power calculations in pwr, you leave out sample size (n) but enter effect size (d), significance level and power:\n\npwr.t.test(n = NULL, d = 0.5, sig.level = 0.05, power = 0.8)\n\n\n     Two-sample t test power calculation \n\n              n = 63.76561\n              d = 0.5\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\nWe need 128 rats.\n\n\nIf you have a good reason to expect a difference in a particular direction, which is whether the treatment group is higher or lower than the control, you set alternative = “greater” or alternative = “less” - it doesn’t matter which you choose for this pwr.t.test command. If you cannot be sure if the treatment will be higher or lower weight, you set alternative = “two.sided”."
  },
  {
    "objectID": "power.html#other-statistical-tests",
    "href": "power.html#other-statistical-tests",
    "title": "Power Analyses",
    "section": "Other Statistical Tests",
    "text": "Other Statistical Tests\nThe pwr package has a bunch of functions, but they all pretty much work the same way.\n\n\n\nFunction\nDescription\n\n\n\n\npwr.2p.test\ntwo proportions (equal n)\n\n\npwr.2p2n.test\ntwo proportions (unequal n)\n\n\npwr.anova.test\nbalanced one way ANOVA\n\n\npwr.chisq.test\nchi-square test\n\n\npwr.f2.test\ngeneral linear model\n\n\npwr.p.test\nproportion (one sample)\n\n\npwr.r.test\ncorrelation\n\n\npwr.t.test\nt-tests (one sample, 2 sample, paired)\n\n\npwr.t2n.test\nt-test (two samples with unequal n)\n\n\n\n\nChallenge\nThe function pwr.anova.test() performs a power analysis for a balanced anova (balanced is when all the groups have the same sample size). k is the number of groups to be compared and f is the effect size.\nYou are planning a project to measure the pollution concentration in fish from three lakes (three groups). Use an effect size of 0.2, significance level of 0.05 and power of 0.8. How many fish do you need to catch from each lake?\n\n\n\nalt.text=A view looking into swallow water with 4 trout swimming.\n\n\n\n\nAnswer\n\n\npwr.anova.test(f=0.2,k=3,power=0.80,sig.level=0.05)\n\n\n     Balanced one-way analysis of variance power calculation \n\n              k = 3\n              n = 81.29603\n              f = 0.2\n      sig.level = 0.05\n          power = 0.8\n\nNOTE: n is number in each group\n\n\nYou need to catch 81 fish from each lake, 243 in total."
  },
  {
    "objectID": "power.html#choosing-an-effect-size-before-the-experiment",
    "href": "power.html#choosing-an-effect-size-before-the-experiment",
    "title": "Power Analyses",
    "section": "Choosing an Effect Size Before the Experiment",
    "text": "Choosing an Effect Size Before the Experiment\nIf you really have nothing else to go on, assume an effect size of 0.5. However, you can normally do better than that, by looking at previous experiments you, or other people, have run.\nKeep in mind that specifying effect size is not a statistical question, it’s an ecological question of what effect size is meaningful for your particular study? For example, do you want to be able to detect a 25% decline in the abundance of a rare animal or would you be happy detecting a 1% decline? For more explanation read the blog post The Effect Size: The Most difficult Step in Calculating Sample Size Estimates."
  },
  {
    "objectID": "power.html#power-analysis-for-estimating-power",
    "href": "power.html#power-analysis-for-estimating-power",
    "title": "Power Analyses",
    "section": "Power Analysis for Estimating Power",
    "text": "Power Analysis for Estimating Power\nImagine this experiment has already taken place. A new treatment was tested on 40 mice (20 in the control group and 20 in the treatment group) and measurements of success taken. The effect size was found to be 0.3.\nChallenge\nUse the function pwr.t.test and calculate what power the t test had. Use a significance level of 0.05.\n\n\nAnswer\n\n\npwr.t.test(n = 20, d = 0.3, sig.level = 0.05, power = NULL)\n\n\n     Two-sample t test power calculation \n\n              n = 20\n              d = 0.3\n      sig.level = 0.05\n          power = 0.1522683\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\nPower was only about 15%. This means that given the effect size, and sample size, we only detected that effect 15% of the time. So, it probably was not worth doing this experiment!\n\n\nHow big would the sample sizes in the experiment above have had to be to achieve 80% power? We can try n = 30:\n\npwr.t.test(n = 30, d = 0.3, sig.level = 0.05, power = NULL)\n\n\n     Two-sample t test power calculation \n\n              n = 30\n              d = 0.3\n      sig.level = 0.05\n          power = 0.2078518\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\nPower improves to around 20%. We need to try higher sample sizes. Instead of manually plugging in different values for n, we could make R run the power analysis for many different sample sizes. This code calculates and plots power for samples sizes from 2 to 200.\n\nnvals <- seq(2, 200, length.out = 200)\npowvals <- sapply(nvals, function(x) pwr.t.test(d = 0.3, n = x, sig.level = 0.05)$power)\nplot(nvals, powvals,\n  xlab = \"sample size\", ylab = \"power\",\n  main = \"Power curve for sample size for difference in proportions\",\n  lwd = 2, col = \"red\", type = \"l\"\n)\nabline(h = 0.8)\n\n\n\n\n\n\nExplanation of code\n\nnvals is an object made to store a sequence seq() of numbers from 2 to 200\npowvals is an object that will store the calculated powers retrieved using $power\nsapply() takes each number x in nvals and uses it in the function pwr.t.test()\nplot() graphs the numbers stored in nvals and powvals against each other\nxlab, ylab and main are the x and y axes labels and main plot title\nlwd is line width, col is line colour and type =1 is a solid line\nabline() draws horizontal (h) line at 0.8\n\n\nNow we can see that a sample size of around 175 for each group would have given enough power.\n\n\nEffect sizes should be reported in results. If a effect size is not given, it can sometimes be calculated. For example, d can be calculated if the means and standard deviations are given."
  }
]